{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph NN for MMWave Sensor filtering\n",
    "The idea is to train a classifier to distinguish between fake points and actual ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Requirements:\n",
    "     * TF:      2.7.0\n",
    "     * Keras:   2.7.0\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "'''\n",
    "    Setting Up matplotlib for paper compliant figures\n",
    "    (this should avoid problems when compiling latex stuff)\n",
    "'''\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Where datasets are stored\n",
    "run_path = \"data/\"\n",
    "experiments_path = \"experiments/\"\n",
    "\n",
    "def load_datasets(init_path):\n",
    "    '''\n",
    "        Return dataset as list of pointclouds\n",
    "        ---\n",
    "        Parameters:\n",
    "        * init_path: string; path to folder holding folders of datasets\n",
    "                     (expected folder structure: 'init_path/run_X/labelled_mmw_run_X.json')\n",
    "    '''\n",
    "    data = []\n",
    "    for run in sorted(os.listdir(init_path)):\n",
    "        if \"run_\" in run:\n",
    "            data.extend(json.load(open(init_path+run+\"/labelled_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "    return data\n",
    "\n",
    "def load_train_test_data(init_path, train_filter=[], test_filter=[]):\n",
    "    '''\n",
    "        Return dataset as list of pointclouds\n",
    "        ---\n",
    "        Parameters:\n",
    "        * init_path: string; path to folder holding folders of datasets\n",
    "                     (expected folder structure: 'init_path/run_X/labelled_mmw_run_X.json')\n",
    "    '''\n",
    "    train = []\n",
    "    test = []\n",
    "    for run in sorted(os.listdir(init_path)):\n",
    "        if \"run_\" in run:\n",
    "            if (int(run.split('_')[-1]) in train_filter):\n",
    "                train.extend(json.load(open(init_path+run+\"/labelled_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "            elif (int(run.split('_')[-1]) in test_filter):\n",
    "                test.extend(json.load(open(init_path+run+\"/labelled_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "    return train, test\n",
    "\n",
    "def get_data_and_label(data, points_per_cloud=200):\n",
    "    '''\n",
    "        Return samples for training as np.array, divided as unlabelled data and related labels.\n",
    "        ---\n",
    "        Parameters:\n",
    "        * data: list of point clouds. (Usually loaded with function load_datasets)\n",
    "        * points_per_cloud: number of points to be found in each point cloud. (Default is 200)\n",
    "    '''\n",
    "    d_x, d_y = [], []\n",
    "    for pc in data:\n",
    "        for i in range(0, len(pc), points_per_cloud): \n",
    "            if len(pc[i:i+points_per_cloud]) == points_per_cloud:\n",
    "                t_ = np.array(pc[i:i+points_per_cloud])[:, :3]\n",
    "                d_x.append(t_)\n",
    "                d_y.append(np.array(pc[i:i+points_per_cloud], dtype=np.float32)[:, -1])\n",
    "                # d_y.append(tf.one_hot(np.array(pc[i:i+points_per_cloud], dtype=np.float32)[:, -1], 2)) # One Hotted\n",
    "    d_x, d_y = np.stack(d_x), np.stack(d_y)\n",
    "    return d_x, d_y\n",
    "\n",
    "train_filter=[4,6,8,9,10,11,12,50,53,54,56,58,59,61,62,63,65,66,67,69]\n",
    "test_filter=[3,7,49,51,55,57,64,68,70]\n",
    "\n",
    "# dataset = load_datasets(run_path)\n",
    "train, test = load_train_test_data(run_path, train_filter=train_filter, test_filter=test_filter)\n",
    "\n",
    "# Separate Train and Test data\n",
    "# d_len = int(len(dataset)*0.7)\n",
    "# train, test = dataset[:d_len], dataset[d_len:]\n",
    "\n",
    "\n",
    "# Shuffle point cloud dataset\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(test)\n",
    "\n",
    "# Get X and Y data for training\n",
    "train_x, train_y = get_data_and_label(train)\n",
    "test_x, test_y = get_data_and_label(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Graph CNN \n",
    "#### https://github.com/WangYueFt/dgcnn\n",
    "#### https://stackoverflow.com/questions/37009647/compute-pairwise-distance-in-a-batch-without-replicating-tensor-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_conv2d(inputs,\n",
    "            filters,\n",
    "            kernel_size,\n",
    "            stride=[1, 1],\n",
    "            padding='SAME',\n",
    "            use_xavier=True,\n",
    "            stddev=1e-3,\n",
    "            activation_fn=tf.nn.elu,\n",
    "            bn=False):\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters, \n",
    "        kernel_size, \n",
    "        strides=stride, \n",
    "        padding=padding,\n",
    "        activation=activation_fn,\n",
    "        kernel_initializer='glorot_uniform' if use_xavier else keras.initializers.TruncatedNormal(stddev=stddev),\n",
    "        bias_initializer='zeros'\n",
    "    )(inputs)\n",
    "\n",
    "    if bn: x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    return x\n",
    "\n",
    "def gnn_dense(inputs,\n",
    "            units,\n",
    "            use_xavier=True,\n",
    "            stddev=1e-3,\n",
    "            activation_fn=tf.nn.elu,\n",
    "            bn=False):\n",
    "            \n",
    "    x = layers.Dense(units,\n",
    "        activation=activation_fn,\n",
    "        kernel_initializer='glorot_uniform' if use_xavier else keras.initializers.TruncatedNormal(stddev=stddev),\n",
    "        bias_initializer='zeros'\n",
    "    )(inputs)\n",
    "\n",
    "    if bn: x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    return x\n",
    "\n",
    "def lambda_get_adj_matr(input):\n",
    "    pcT = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(input)\n",
    "    pc_inn = layers.Lambda(lambda x: tf.matmul(x[0], x[1]))( (input, pcT) )\n",
    "    pc2 = layers.Lambda(lambda x: tf.reduce_sum(tf.square(x), axis=-1, keepdims=True))(input)\n",
    "    pc2T = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(pc2)\n",
    "    output = layers.Lambda(lambda x: x[0] + -2 * x[1] + x[2])( (pc2, pc_inn, pc2T) )\n",
    "    # Uncomment line below to use reciprocal of adj matrix (1/distance)\n",
    "    # output = layers.Lambda(lambda x: tf.math.reciprocal(x))(output)\n",
    "    return output\n",
    "\n",
    "def lambda_knn(adj, k=20):\n",
    "    x = layers.Lambda(lambda x: tf.math.top_k(-x[0], x[1]))( (adj, k) )\n",
    "    return x.indices\n",
    "\n",
    "def lambda_edge_feature(inputs, nn_idxs, k=20, num_points=200, num_dims=3):\n",
    "\n",
    "    pc_central = inputs\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    idx_ = layers.Lambda(lambda x: tf.range(x[0]) * x[1])( (batch_size, num_points) )\n",
    "    idx_ = layers.Lambda(lambda x: tf.reshape(x[0], (x[1], 1, 1)))( (idx_, batch_size) )\n",
    "    # Adding to list of idxs of k points the points themselves\n",
    "    pc_temp1 = layers.Lambda(lambda x: x[0]+x[1])( (nn_idxs, idx_) )\n",
    "\n",
    "    # Flattening of points into a list of coordinates (x,y,z)\n",
    "    pc_flat = layers.Lambda(lambda x: tf.reshape(x[0], [-1, x[1]]))( (inputs, num_dims) )\n",
    "\n",
    "    # Collect points from computed idxs\n",
    "    pc_neighbors = layers.Lambda(lambda x: tf.gather(x[0], x[1]) )( (pc_flat, pc_temp1) )\n",
    "\n",
    "    # Reshape points into shape (batch, num_points, NEW_AXIS = 1, num_dims)\n",
    "    pc_central = layers.Lambda(lambda x: tf.expand_dims(x, axis=-2))(pc_central)\n",
    "    # Points are repeated k-times along new dimension ==> (batch, num_points, k, num_dims)\n",
    "    pc_central = layers.Lambda(lambda x: tf.tile(x[0], [1, 1, x[1], 1]))( (pc_central, k) )\n",
    "\n",
    "    pc_temp2 = layers.Lambda(lambda x: tf.subtract(x[0], x[1]))( (pc_neighbors, pc_central) )\n",
    "    edge_feature = layers.Lambda(lambda x: tf.concat((x[0], x[1]), axis=-1))((pc_central, pc_temp2))\n",
    "    return edge_feature\n",
    "\n",
    "def gnn_tnet(inputs, num_dims, tnet_shapes, bn=False):\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "    for filt in tnet_shapes[0]:\n",
    "        x = gnn_conv2d(inputs, filters=filt, kernel_size=[1,1], bn=bn)\n",
    "    x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "    for filt in tnet_shapes[1]:\n",
    "        x = gnn_conv2d(inputs, filters=filt, kernel_size=[1,1], bn=bn)\n",
    "    x = layers.GlobalMaxPooling2D(keepdims=True)(x)\n",
    "    x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], y[2])))( [x, batch_size, x.shape[-1]] )\n",
    "\n",
    "    for neur in tnet_shapes[2]:\n",
    "        x = gnn_dense(x, neur, bn)\n",
    "    \n",
    "    bias = keras.initializers.Constant(np.eye(num_dims).flatten())\n",
    "    x = layers.Dense(\n",
    "        num_dims * num_dims,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_dims, num_dims))(x)\n",
    "    return feat_T\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "# test_name = \"test_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test custom losses\n",
    "def custom_loss(pred, labels):\n",
    "    # loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=labels, logits=pred, label_smoothing=0.2)\n",
    "    # classify_loss = tf.reduce_mean(loss)\n",
    "    # loss = tf.reduce_mean(tf.reduce_sum(tf.math.square(tf.math.subtract(labels, pred)), axis=-1))\n",
    "    loss = tf.reduce_mean(tf.keras.losses.Huber()(labels, pred))\n",
    "    return loss\n",
    "\n",
    "# Callback to save good models. Threshold is on validation accuracy.\n",
    "class ValAccThresh_CB(keras.callbacks.Callback):\n",
    "    def __init__(self, thresh=0.85, experiments_path=\"experiments/\", test_name=\"test\"):\n",
    "        self.thresh = thresh\n",
    "        super(keras.callbacks.Callback, self).__init__()\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "        self.stopped_epoch = 0\n",
    "        self.experiments_path = experiments_path\n",
    "        self.test_name = test_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # self.current_epoch += 1\n",
    "        val_key = \"\"\n",
    "        for k in logs.keys():\n",
    "            if \"val\" in k and \"accuracy\" in k:\n",
    "                val_key = k\n",
    "                break\n",
    "        if val_key == \"\": print(\" Validation Accuracy key not found.\")\n",
    "\n",
    "        current = logs.get(val_key)\n",
    "        # current = logs.get(\"val_sparse_categorical_accuracy\")\n",
    "        # current = logs.get(\"val_accuracy\")\n",
    "        if current >= self.thresh:\n",
    "            self.thresh = current\n",
    "            self.model.save_weights(self.experiments_path+self.test_name+\"/best_weights/cp-\"+str(epoch)+\".ckpt\")\n",
    "            print(\" New good model saved.\")\n",
    "\n",
    "# Callback to save history for post-processing\n",
    "# filename=experiments_path+test_name+\"/history.csv\"\n",
    "# history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(inputs, \n",
    "        num_points, num_dims, k,\n",
    "        tnet_shape,\n",
    "        conv_gnns,\n",
    "        dense_gnn,\n",
    "        classes=2):\n",
    "    '''\n",
    "        Returns the outputs of the model to be compiled.\n",
    "        ---\n",
    "        Arguments: \n",
    "        * inputs:       Expected (None, 3). instance of tf.Input.\n",
    "        * num_points:   Number of points per point cloud. Default is 200\n",
    "        * num_dims:     Number of dimensions per point. Default is 3 (x, y, z)\n",
    "        * k:            K nearest neighbors\n",
    "        * tnet_shape:   Array of three lists. (each list's length is the number of layers for that section)\n",
    "                        1st is a list of filters for convolutional layers before reduce_max.\n",
    "                        2nd is a list of filters for convolutional layers after reduce_max.\n",
    "                        3rd is a list of neurons for dense layers after max pooling.\n",
    "        * conv_gnns:    list. Each row is composed of two lists.\n",
    "                        1st is a list of filters for convolutional layers before computing edge features.\n",
    "                        2nd is a list of filters for convolutional layers after computing edge features.\n",
    "        * dense_gnn:    list of neuorns for dense layers at the end of the network.\n",
    "        * classes:      number of classes to classify.\n",
    "    '''\n",
    "\n",
    "    adj = lambda_get_adj_matr(inputs)\n",
    "    nn_idxs = lambda_knn(adj, k)\n",
    "    edge_feats = lambda_edge_feature(inputs, nn_idxs, k, num_points, num_dims)\n",
    "    feat_T = gnn_tnet(edge_feats, num_dims, tnet_shape, bn=False)\n",
    "    pc_tf = layers.Dot(axes=(-1, -2))([inputs, feat_T]) # Apply affine transformation to input features\n",
    "\n",
    "    adj = lambda_get_adj_matr(pc_tf)\n",
    "    nn_idxs = lambda_knn(adj, k)\n",
    "    edge_feats = lambda_edge_feature(pc_tf, nn_idxs, k, num_points, num_dims)\n",
    "\n",
    "    for l in conv_gnns:\n",
    "        x = edge_feats\n",
    "        for gc_filt in l[0]:\n",
    "            x = gnn_conv2d(x, gc_filt, [1,1], bn=False)\n",
    "        x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "        x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[0][-1])))( [x, tf.shape(inputs)[0]] )\n",
    "\n",
    "        adj = lambda_get_adj_matr(x)\n",
    "        nn_idxs = lambda_knn(adj, k)\n",
    "        edge_feats = lambda_edge_feature(x, nn_idxs, k, num_points, l[0][-1])\n",
    "        x = edge_feats\n",
    "        for gc_filt in l[1]:\n",
    "            x = gnn_conv2d(x, gc_filt, [1,1], bn=False)\n",
    "        x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "        x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[1][-1])))( [x, tf.shape(inputs)[0]] )\n",
    "\n",
    "    for w_ in dense_gnn:\n",
    "        x = gnn_dense(x, w_, bn=False)\n",
    "\n",
    "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    return outputs\n",
    "\n",
    "def objective(trial):\n",
    "    test_name = \"fixed_test2_\"+str(trial.number)\n",
    "    filename=experiments_path+test_name+\"/history.csv\"\n",
    "    history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "    ############################ HyperParameter Setup ############################\n",
    "    ######################### Check build_model for docs #########################\n",
    "    k = trial.suggest_int('k', 5,75) #30\n",
    "    batch_size = trial.suggest_int('batch_size', 8,128) #16\n",
    "    # tnet_before_max = trial.suggest_int('tnet_before_max', 1,3)\n",
    "    # tnet_before = []\n",
    "    # for i in range(tnet_before_max):\n",
    "    #     tnet_before.append(trial.suggest_int('tnet_beforemax_layer_'+str(i), 8,128))\n",
    "    # tnet_after = []\n",
    "    # tnet_after_max = trial.suggest_int('tnet_after_max', 1,3)\n",
    "    # for i in range(tnet_after_max):\n",
    "    #     tnet_after.append(trial.suggest_int('tnet_aftermax_layer_'+str(i), 8,128))\n",
    "    # tnet_dense = []\n",
    "    # tnet_dense_layers = trial.suggest_int('tnet_dense_layers', 1,3)\n",
    "    # for i in range(tnet_dense_layers):\n",
    "    #     tnet_dense.append(trial.suggest_int('tnet_dense_layer_'+str(i), 16,256))\n",
    "    tnet_before = [32]\n",
    "    tnet_after = [128]\n",
    "    tnet_dense = [128, 128]\n",
    "    tnet_shape = [tnet_before, tnet_after, tnet_dense]\n",
    "\n",
    "    # gc_layers = trial.suggest_int('gc_layers', 1,3) #1\n",
    "    conv_gnns = []\n",
    "    # for _ in range(gc_layers):\n",
    "    #     before_edge_gcl = trial.suggest_int('before_edge_gcl', 1,3) #2\n",
    "    #     after_edge_gcl = trial.suggest_int('after_edge_gcl', 1,3) #2\n",
    "\n",
    "    #     bfr_edge = []\n",
    "    #     for i in range(before_edge_gcl):\n",
    "    #         bfr_edge.append(trial.suggest_int('before_edge_gcl_'+str(i), 8,128))\n",
    "            \n",
    "    #     aft_edge = []\n",
    "    #     for i in range(after_edge_gcl):\n",
    "    #         aft_edge.append(trial.suggest_int('after_edge_gcl_'+str(i), 8,128))\n",
    "            \n",
    "    #     conv_gnns.append([bfr_edge, aft_edge])\n",
    "    bfr_edge = [32, 64]\n",
    "    aft_edge = [128]\n",
    "    conv_gnns.append([bfr_edge, aft_edge])\n",
    "    # dense_layers = trial.suggest_int('dense_layers', 1,3)\n",
    "    # dense_gnn = []\n",
    "    # for i in range(dense_layers):\n",
    "    #     dense_gnn.append(trial.suggest_int('dense_layer_'+str(i), 16,256))\n",
    "    dense_gnn = [256, 128]\n",
    "        \n",
    "    lr = trial.suggest_float('lr', 0.0001, 0.1)\n",
    "    steps_per_epoch=trial.suggest_int('steps_per_epoch', 15,100)\n",
    "\n",
    "    validation_steps=25     # Static\n",
    "    num_points = 200        # Static\n",
    "    num_dims = 3            # Static\n",
    "    ##############################################################################\n",
    "\n",
    "    inputs = keras.Input(shape=(None, num_dims))\n",
    "    \n",
    "    outputs = build_model(inputs,\n",
    "                    num_points, num_dims, k,\n",
    "                    tnet_shape,\n",
    "                    conv_gnns,\n",
    "                    dense_gnn\n",
    "                )\n",
    "    model = keras.Model(inputs=[inputs], outputs=outputs, name=\"gnn_pointnet\")\n",
    "\n",
    "    opt_pi = tf.optimizers.Adam(learning_rate =  lr )\n",
    "    # opt_pi = tf.optimizers.RMSprop(learning_rate =  lr )\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=opt_pi, metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    # model.compile(loss=tf.nn.sparse_softmax_cross_entropy_with_logits , optimizer=opt_pi, metrics=['accuracy'])\n",
    "    # model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=opt_pi, metrics=['accuracy'])\n",
    "    # model.compile(loss=custom_loss, optimizer=opt_pi, metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_path = experiments_path+test_name+\"/cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, \n",
    "        verbose=0, \n",
    "        save_weights_only=False,\n",
    "        save_freq=10*batch_size)\n",
    "        \n",
    "    latest = tf.train.latest_checkpoint(experiments_path+test_name+\"/\")\n",
    "    if latest:\n",
    "        model.load_weights(latest)\n",
    "        latest_ep = int(latest.split('/')[-1].split('-')[-1].split('.')[0])\n",
    "        print(\" Model loaded correctly:\", latest, \" - Epoch \", latest_ep)\n",
    "    else:\n",
    "        print(\" The model could not be loaded properly: \", latest)\n",
    "        model.save(checkpoint_path.format(epoch=0))\n",
    "        latest_ep = 0\n",
    "\n",
    "    # Use CPU as default due to GPU's memory issues\n",
    "    with tf.device('/CPU:0'):\n",
    "        history = model.fit(\n",
    "            train_x, \n",
    "            train_y, \n",
    "            \n",
    "            initial_epoch=latest_ep,\n",
    "            batch_size=batch_size, \n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=validation_steps,\n",
    "\n",
    "            validation_split=0.3,\n",
    "            epochs=150 - latest_ep, # Train for 150 epochs to find the configuration that can later be trained for more epochs.\n",
    "            shuffle=True,\n",
    "            callbacks=[ValAccThresh_CB(thresh=0.9), cp_callback, history_logger],\n",
    "            use_multiprocessing=False,\n",
    "            workers=8,\n",
    "        )\n",
    "    return np.mean(history.history['val_sparse_categorical_accuracy'][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually train\n",
    "storage = optuna.storages.RDBStorage(url=\"sqlite:///gnn_fixed2.db\", engine_kwargs={\"connect_args\": {\"timeout\": 5}})\n",
    "study = optuna.create_study(study_name=\"gnn_denoising_fixed2\", storage=storage, load_if_exists=True, direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Configurations Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The model could not be loaded properly:  None\n",
      "INFO:tensorflow:Assets written to: experiments/manual_reboot_test_00/cp-0000.ckpt/assets\n",
      "Epoch 1/20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 12:01:45.808331: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 61440000 exceeds 10% of free system memory.\n",
      "2023-10-09 12:01:45.843887: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 327680000 exceeds 10% of free system memory.\n",
      "2023-10-09 12:01:46.069922: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 327680000 exceeds 10% of free system memory.\n",
      "2023-10-09 12:01:46.069999: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 327680000 exceeds 10% of free system memory.\n",
      "2023-10-09 12:01:46.477124: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 81920000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mSi è verificato un arresto anomalo del kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. Esaminare il codice nelle celle per identificare una possibile causa dell'errore. Per altre informazioni, fare clic su <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a>. Per altri dettagli, vedere Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "test_name = \"manual_reboot_test_00\"\n",
    "\n",
    "k=200\n",
    "batch_size=64\n",
    "steps_per_epoch=32\n",
    "validation_steps=25\n",
    "lr = 0.001\n",
    "\n",
    "num_points=200\n",
    "num_dims=3\n",
    "\n",
    "checkpoint_path = experiments_path+test_name+\"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=0,\n",
    "    save_weights_only=False,\n",
    "    save_freq=10*steps_per_epoch)\n",
    "\n",
    "inputs = keras.Input(shape=(None, 3))\n",
    "\n",
    "tnet_shape = [[32], [32], [64,64]]\n",
    "conv_gnns = [[[128,128,256], [256, 64]]]\n",
    "dense_gnn = [128,64]\n",
    "\n",
    "outputs = build_model(inputs,\n",
    "                    num_points, num_dims, k,\n",
    "                    tnet_shape,\n",
    "                    conv_gnns,\n",
    "                    dense_gnn\n",
    "                )\n",
    "\n",
    "model = keras.Model(inputs=[inputs], outputs=outputs, name=test_name+\"net\")\n",
    "\n",
    "\n",
    "opt_pi = tf.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=opt_pi, metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# Try to load the model. If it does not exist, create it.\n",
    "latest = tf.train.latest_checkpoint(experiments_path+test_name+\"/\")\n",
    "if latest:\n",
    "    # https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "    # model.load(latest)\n",
    "    model = tf.keras.models.load_model(latest)\n",
    "    latest_ep = int(latest.split('/')[-1].split('-')[-1].split('.')[0])\n",
    "    print(\" Model loaded correctly:\", latest, \" - Epoch \", latest_ep)\n",
    "else:\n",
    "    print(\" The model could not be loaded properly: \", latest)\n",
    "    model.save(checkpoint_path.format(epoch=0))\n",
    "    latest_ep = 0\n",
    "\n",
    "# This grants no overwriting of the history file\n",
    "filename=experiments_path+test_name+\"/history\"+str(latest_ep)+\".csv\"\n",
    "history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "\n",
    "# Use CPU as default due to GPU's memory issues\n",
    "with tf.device('/CPU:0'):\n",
    "    history = model.fit(\n",
    "        train_x, \n",
    "        train_y, \n",
    "        \n",
    "        initial_epoch=latest_ep,\n",
    "        batch_size=batch_size, \n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "\n",
    "        validation_split=0.3,\n",
    "        epochs=20000,\n",
    "        shuffle=True,\n",
    "        callbacks=[ValAccThresh_CB(thresh=0.75, experiments_path=experiments_path, test_name=test_name), cp_callback, history_logger],\n",
    "        use_multiprocessing=False,\n",
    "        workers=8,\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
