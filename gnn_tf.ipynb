{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph NN for MMWave Sensor filtering\n",
    "The idea is to train a classifier to distinguish between fake points and actual ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 17:05:41.749990: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-04-03 17:05:41.750025: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data shape:  (7013816, 6)\n",
      "Points per cloud:  200\n",
      "Skipping because:  (16, 6) 7013800 7014000\n",
      "Skipped 1 point clouds\n",
      " Data shape:  (1148223, 6)\n",
      "Points per cloud:  200\n",
      "Skipping because:  (23, 6) 1148200 1148400\n",
      "Skipped 1 point clouds\n",
      " Data shape:  (3411821, 6)\n",
      "Points per cloud:  200\n",
      "Skipping because:  (21, 6) 3411800 3412000\n",
      "Skipped 1 point clouds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Requirements:\n",
    "     * TF:      2.7.0\n",
    "     * Keras:   2.7.0\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "'''\n",
    "    Setting Up matplotlib for paper compliant figures\n",
    "    (this should avoid problems when compiling latex stuff)\n",
    "'''\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Where datasets are stored\n",
    "run_path = \"data/\"\n",
    "experiments_path = \"experiments/\"\n",
    "\n",
    "def load_datasets(init_path):\n",
    "    '''\n",
    "        Return dataset as list of pointclouds\n",
    "        ---\n",
    "        Parameters:\n",
    "        * init_path: string; path to folder holding folders of datasets\n",
    "                     (expected folder structure: 'init_path/run_X/labelled_mmw_run_X.json')\n",
    "    '''\n",
    "    data = []\n",
    "    for run in sorted(os.listdir(init_path)):\n",
    "        if \"run_\" in run:\n",
    "            data.extend(json.load(open(init_path+run+\"/labelled_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "    return data\n",
    "\n",
    "def load_train_test_data(init_path, train_filter=[], valid_filter=[], test_filter=[]):\n",
    "    '''\n",
    "        Return dataset as list of pointclouds\n",
    "        ---\n",
    "        Parameters:\n",
    "        * init_path: string; path to folder holding folders of datasets\n",
    "                     (expected folder structure: 'init_path/run_X/labelled_mmw_run_X.json')\n",
    "    '''\n",
    "    train = []\n",
    "    valid = []\n",
    "    test = []\n",
    "    for run in sorted(os.listdir(init_path)):\n",
    "        if \"run_\" in run:\n",
    "            if (int(run.split('_')[-1]) in train_filter):\n",
    "                # train.extend(json.load(open(init_path+run+\"/labelled_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "                train.extend(json.load(open(init_path+run+\"/norm_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "            elif (int(run.split('_')[-1]) in valid_filter):\n",
    "                valid.extend(json.load(open(init_path+run+\"/labelled_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "            elif (int(run.split('_')[-1]) in test_filter):\n",
    "                test.extend(json.load(open(init_path+run+\"/labelled_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "    return train, valid, test\n",
    "\n",
    "def get_data_and_label(data, points_per_cloud=200):\n",
    "    '''\n",
    "        Return samples for training as np.array, divided as unlabelled data and related labels.\n",
    "        ---\n",
    "        Parameters:\n",
    "        * data: list of point clouds. (Usually loaded with function load_datasets)\n",
    "        * points_per_cloud: number of points to be found in each point cloud. (Default is 200)\n",
    "    '''\n",
    "    d_x, d_y = [], []\n",
    "    skipped = 0\n",
    "    for pc in data:\n",
    "        for i in range(0, len(pc), points_per_cloud): \n",
    "            if len(pc[i:i+points_per_cloud]) == points_per_cloud:\n",
    "                t_ = np.array(pc[i:i+points_per_cloud])[:, :3]\n",
    "                d_x.append(t_)\n",
    "                d_y.append(np.array(pc[i:i+points_per_cloud], dtype=np.float32)[:, -1])\n",
    "                # d_y.append(tf.one_hot(np.array(pc[i:i+points_per_cloud], dtype=np.float32)[:, -1], 2)) # One Hotted\n",
    "            else:\n",
    "                skipped += 1\n",
    "    d_x, d_y = np.stack(d_x), np.stack(d_y)\n",
    "    print(\"Skipped {} point clouds\".format(skipped))\n",
    "    return d_x, d_y\n",
    "\n",
    "def get_data_and_label2(data, points_per_cloud=200):\n",
    "    '''\n",
    "        Return samples for training as np.array, divided as unlabelled data and related labels.\n",
    "        ---\n",
    "        Parameters:\n",
    "        * data: list of point clouds. (Usually loaded with function load_datasets)\n",
    "        * points_per_cloud: number of points to be found in each point cloud. (Default is 200)\n",
    "    '''\n",
    "    d_x, d_y = [], []\n",
    "    skipped = 0\n",
    "    data = np.vstack(data)\n",
    "    print(\" Data shape: \", data.shape)\n",
    "    # for pc in data:\n",
    "    for i in range(0, len(data), points_per_cloud): \n",
    "        if len(data[i:i+points_per_cloud]) == points_per_cloud:\n",
    "            t_ = np.array(data[i:i+points_per_cloud])[:, :3]\n",
    "            d_x.append(t_)\n",
    "            d_y.append(np.array(data[i:i+points_per_cloud], dtype=np.float32)[:, -1])\n",
    "            # d_y.append(tf.one_hot(np.array(pc[i:i+points_per_cloud], dtype=np.float32)[:, -1], 2)) # One Hotted\n",
    "        else:\n",
    "            print(\"Points per cloud: \", points_per_cloud)\n",
    "            print(\"Skipping because: \", data[i:i+points_per_cloud].shape, i, i+points_per_cloud)\n",
    "            skipped += 1\n",
    "        #     break\n",
    "        # break\n",
    "    d_x, d_y = np.stack(d_x), np.stack(d_y)\n",
    "    print(\"Skipped {} point clouds\".format(skipped))\n",
    "    return d_x, d_y\n",
    "\n",
    "valid_filter = [9, 69, 73, 3, 55]\n",
    "train_filter=[4, 5, 7, 8, 49, 51, 54, 58, 59, 62, 63, 65, 66, 68, 71, 72, 75, 76, 77, 79, 81, 82, 84, 86, 87, 89, 90, 93, 95, 97, 98]\n",
    "test_filter=[53, 67, 74, 80, 88, 96, 61, 6, 64, 92, 70, 85, 50, 56, 57]\n",
    "\n",
    "\n",
    "# Original separation\n",
    "# train_filter=[4,6,8,9,50,51,53,54,56,58,59,61,62,63,65,66,67,69]\n",
    "# test_filter=[3,7,49,55,57,64,68,70]\n",
    "\n",
    "# dataset = load_datasets(run_path)\n",
    "train, valid, test = load_train_test_data(run_path, train_filter=train_filter, valid_filter=valid_filter, test_filter=test_filter)\n",
    "\n",
    "# Separate Train and Test data\n",
    "# d_len = int(len(dataset)*0.7)\n",
    "# train, test = dataset[:d_len], dataset[d_len:]\n",
    "\n",
    "\n",
    "# Shuffle point cloud dataset\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(valid)\n",
    "np.random.shuffle(test)\n",
    "\n",
    "# Get X and Y data for training\n",
    "train_x, train_y = get_data_and_label2(train)\n",
    "valid_x, valid_y = get_data_and_label2(valid)\n",
    "test_x, test_y = get_data_and_label2(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35069, 5741, 17059)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x), len(valid_x), len(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Graph CNN \n",
    "#### https://github.com/WangYueFt/dgcnn\n",
    "#### https://stackoverflow.com/questions/37009647/compute-pairwise-distance-in-a-batch-without-replicating-tensor-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_conv2d(inputs,\n",
    "            filters,\n",
    "            kernel_size,\n",
    "            stride=[1],\n",
    "            padding='SAME',\n",
    "            use_xavier=True,\n",
    "            stddev=1e-3,\n",
    "            activation_fn=tf.nn.elu,\n",
    "            bn=False):\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters, \n",
    "        kernel_size, \n",
    "        strides=stride, \n",
    "        padding=padding,\n",
    "        activation=activation_fn,\n",
    "        kernel_initializer='glorot_uniform' if use_xavier else keras.initializers.TruncatedNormal(stddev=stddev),\n",
    "        bias_initializer='zeros'\n",
    "    )(inputs)\n",
    "\n",
    "    if bn: x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    return x\n",
    "\n",
    "def gnn_dense(inputs,\n",
    "            units,\n",
    "            use_xavier=True,\n",
    "            stddev=1e-3,\n",
    "            activation_fn=tf.nn.elu,\n",
    "            bn=False):\n",
    "            \n",
    "    x = layers.Dense(units,\n",
    "        activation=activation_fn,\n",
    "        kernel_initializer='glorot_uniform' if use_xavier else keras.initializers.TruncatedNormal(stddev=stddev),\n",
    "        bias_initializer='zeros'\n",
    "    )(inputs)\n",
    "\n",
    "    if bn: x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    return x\n",
    "\n",
    "def lambda_get_adj_matr(input):\n",
    "    pcT = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(input)\n",
    "    pc_inn = layers.Lambda(lambda x: tf.matmul(x[0], x[1]))( (input, pcT) )\n",
    "    pc2 = layers.Lambda(lambda x: tf.reduce_sum(tf.square(x), axis=-1, keepdims=True))(input)\n",
    "    pc2T = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(pc2)\n",
    "    output = layers.Lambda(lambda x: x[0] + -2 * x[1] + x[2])( (pc2, pc_inn, pc2T) )\n",
    "    # Uncomment line below to use reciprocal of adj matrix (1/distance)\n",
    "    # output = layers.Lambda(lambda x: tf.math.reciprocal(x))(output)\n",
    "    return output\n",
    "\n",
    "def lambda_knn(adj, k=20):\n",
    "    x = layers.Lambda(lambda x: tf.math.top_k(-x[0], x[1]))( (adj, k) )\n",
    "    return x.indices\n",
    "\n",
    "def lambda_edge_feature(inputs, nn_idxs, k=20, num_points=200, num_dims=3):\n",
    "\n",
    "    pc_central = inputs\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    idx_ = layers.Lambda(lambda x: tf.range(x[0]) * x[1])( (batch_size, num_points) )\n",
    "    idx_ = layers.Lambda(lambda x: tf.reshape(x[0], (x[1], 1, 1)))( (idx_, batch_size) )\n",
    "    # Adding to list of idxs of k points the points themselves\n",
    "    pc_temp1 = layers.Lambda(lambda x: x[0]+x[1])( (nn_idxs, idx_) )\n",
    "\n",
    "    # Flattening of points into a list of coordinates (x,y,z)\n",
    "    pc_flat = layers.Lambda(lambda x: tf.reshape(x[0], [-1, x[1]]))( (inputs, num_dims) )\n",
    "\n",
    "    # Collect points from computed idxs\n",
    "    pc_neighbors = layers.Lambda(lambda x: tf.gather(x[0], x[1]) )( (pc_flat, pc_temp1) )\n",
    "\n",
    "    # Reshape points into shape (batch, num_points, NEW_AXIS = 1, num_dims)\n",
    "    pc_central = layers.Lambda(lambda x: tf.expand_dims(x, axis=-2))(pc_central)\n",
    "    # Points are repeated k-times along new dimension ==> (batch, num_points, k, num_dims)\n",
    "    pc_central = layers.Lambda(lambda x: tf.tile(x[0], [1, 1, x[1], 1]))( (pc_central, k) )\n",
    "\n",
    "    pc_temp2 = layers.Lambda(lambda x: tf.subtract(x[0], x[1]))( (pc_neighbors, pc_central) )\n",
    "    edge_feature = layers.Lambda(lambda x: tf.concat((x[0], x[1]), axis=-1))((pc_central, pc_temp2))\n",
    "    return edge_feature\n",
    "\n",
    "class CustomOrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, num_features=3, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.eye = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'num_features': self.num_features, 'l2reg': self.l2reg}\n",
    "\n",
    "def gnn_tnet(inputs, num_dims, tnet_shapes, bn=False):\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "    for filt in tnet_shapes[0]:\n",
    "        x = gnn_conv2d(inputs, filters=filt, kernel_size=[1], bn=bn)\n",
    "    # x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "    for filt in tnet_shapes[1]:\n",
    "        x = gnn_conv2d(inputs, filters=filt, kernel_size=[1], bn=bn)\n",
    "    x = layers.GlobalMaxPooling1D(keepdims=True)(x)\n",
    "    # print(\" [tnet] global maxpool 2d shape: \", x.shape)\n",
    "    x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], y[2])))( [x, batch_size, x.shape[-1]] )\n",
    "    # print(\" [tnet] reshape: \", x.shape)\n",
    "\n",
    "    for neur in tnet_shapes[2]:\n",
    "        x = gnn_dense(x, neur, bn)\n",
    "    \n",
    "    bias = keras.initializers.Constant(np.eye(num_dims).flatten())\n",
    "    reg = CustomOrthogonalRegularizer(num_features=num_dims)\n",
    "    x = layers.Dense(\n",
    "        num_dims * num_dims,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "        activity_regularizer=reg,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_dims, num_dims), name='tnet_last_reshape'+str(np.round(time.time(), 5)))(x)\n",
    "    return feat_T\n",
    "\n",
    "# function to test custom losses\n",
    "def custom_loss(pred, labels):\n",
    "    # loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=labels, logits=pred, label_smoothing=0.2)\n",
    "    # classify_loss = tf.reduce_mean(loss)\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.math.square(tf.math.subtract(labels, pred)), axis=-1))\n",
    "    return loss\n",
    "\n",
    "'''\n",
    "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "tf.math.l2_normalize\n",
    "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "'''\n",
    "\n",
    "# Callback to save good models. Threshold is on validation accuracy.\n",
    "class ValAccThresh_CB(keras.callbacks.Callback):\n",
    "    def __init__(self, thresh=0.85, experiments_path=\"experiments/\", test_name=\"test\"):\n",
    "        self.thresh = thresh\n",
    "        super(keras.callbacks.Callback, self).__init__()\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "        self.stopped_epoch = 0\n",
    "        self.experiments_path = experiments_path\n",
    "        self.test_name = test_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # self.current_epoch += 1\n",
    "        val_key = \"\"\n",
    "        for k in logs.keys():\n",
    "            if \"val\" in k and \"accuracy\" in k:\n",
    "                val_key = k\n",
    "                break\n",
    "        assert logs.get(val_key) != None, print(\" Validation Accuracy not found\", self.thresh, logs.get(val_key), val_key, logs.get(val_key) == None)\n",
    "\n",
    "        current = logs.get(val_key)\n",
    "        # current = logs.get(\"val_sparse_categorical_accuracy\")\n",
    "        # current = logs.get(\"val_accuracy\")\n",
    "        if current >= self.thresh:\n",
    "            self.thresh = current\n",
    "            self.model.save_weights(self.experiments_path+self.test_name+\"/best_weights/cp-\"+str(epoch)+\".ckpt\")\n",
    "            print(\" New good model saved.\")\n",
    "\n",
    "# Callback to save history for post-processing\n",
    "# filename=experiments_path+test_name+\"/history.csv\"\n",
    "# history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "# test_name = \"test_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(inputs, \n",
    "        num_points, num_dims, k,\n",
    "        tnet_shape,\n",
    "        conv_gnns,\n",
    "        dense_gnn,\n",
    "        classes=2):\n",
    "    '''\n",
    "        Returns the outputs of the model to be compiled.\n",
    "        ---\n",
    "        Arguments: \n",
    "        * inputs:       Expected (None, 3). instance of tf.Input.\n",
    "        * num_points:   Number of points per point cloud. Default is 200\n",
    "        * num_dims:     Number of dimensions per point. Default is 3 (x, y, z)\n",
    "        * k:            K nearest neighbors\n",
    "        * tnet_shape:   Array of three lists. (each list's length is the number of layers for that section)\n",
    "                        1st is a list of filters for convolutional layers before reduce_max.\n",
    "                        2nd is a list of filters for convolutional layers after reduce_max.\n",
    "                        3rd is a list of neurons for dense layers after max pooling.\n",
    "        * conv_gnns:    list. Each row is composed of two lists.\n",
    "                        1st is a list of filters for convolutional layers before computing edge features.\n",
    "                        2nd is a list of filters for convolutional layers after computing edge features.\n",
    "        * dense_gnn:    list of neuorns for dense layers at the end of the network.\n",
    "        * classes:      number of classes to classify.\n",
    "    '''\n",
    "\n",
    "    # adj = lambda_get_adj_matr(inputs)\n",
    "    # nn_idxs = lambda_knn(adj, k)\n",
    "    # edge_feats = lambda_edge_feature(inputs, nn_idxs, k, num_points, num_dims)\n",
    "    feat_T = gnn_tnet(inputs, num_dims, tnet_shape[0], bn=True)\n",
    "    # print(\" [main] tnet shape: \", inputs.shape, feat_T.shape)\n",
    "    pc_tf = layers.Dot(axes=(-1, -2))([inputs, feat_T]) # Apply affine transformation to input features\n",
    "    # print(\" [main] dot shape: \", pc_tf.shape)\n",
    "\n",
    "    adj = lambda_get_adj_matr(pc_tf)\n",
    "    nn_idxs = lambda_knn(adj, k)\n",
    "    edge_feats = lambda_edge_feature(pc_tf, nn_idxs, k, num_points, num_dims)\n",
    "    x = edge_feats\n",
    "    # x = pc_tf\n",
    "\n",
    "    for l in conv_gnns:\n",
    "\n",
    "        for gc_filt in l[0]:\n",
    "            x = gnn_conv2d(x, gc_filt, [1], bn=True)\n",
    "        \n",
    "        # x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "        # x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[0][-1])))( [x, tf.shape(inputs)[0]] )\n",
    "\n",
    "        # adj = lambda_get_adj_matr(x)\n",
    "        # nn_idxs = lambda_knn(adj, k)\n",
    "        # edge_feats = lambda_edge_feature(x, nn_idxs, k, num_points, l[0][-1])\n",
    "        # x = edge_feats\n",
    "\n",
    "        feat_T = gnn_tnet(x, x.shape[-1], tnet_shape[1], bn=True)\n",
    "        # print(\" [main] tnet SHAPES: \", x.shape, feat_T.shape)\n",
    "        x = layers.Dot(axes=(-1, -2))([x, feat_T])\n",
    "        # print(\" [main] DOT SHAPE: \", x.shape)\n",
    "\n",
    "        # adj = lambda_get_adj_matr(x)\n",
    "        # nn_idxs = lambda_knn(adj, k)\n",
    "        # edge_feats = lambda_edge_feature(x, nn_idxs, k, num_points, x.shape[-1])\n",
    "\n",
    "        for gc_filt in l[1]:\n",
    "            x = gnn_conv2d(x, gc_filt, [1], bn=True)\n",
    "        # x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "        # x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[1][-1])))( [x, tf.shape(inputs)[0]] )\n",
    "\n",
    "    # x = layers.GlobalMaxPooling1D(keepdims=True)(x)\n",
    "    # x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], y[2])))( [x, tf.shape(x)[0], x.shape[-1]] )\n",
    "    \n",
    "    for w_ in dense_gnn:\n",
    "        x = gnn_dense(x, w_, bn=True)\n",
    "\n",
    "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    return outputs\n",
    "\n",
    "def objective(trial):\n",
    "    test_name = \"fixed_test_\"+str(trial.number)\n",
    "    filename=experiments_path+test_name+\"/history.csv\"\n",
    "    history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "    ############################ HyperParameter Setup ############################\n",
    "    ######################### Check build_model for docs #########################\n",
    "    k = trial.suggest_int('k', 5,75) #30\n",
    "    batch_size = trial.suggest_int('batch_size', 8,128) #16\n",
    "    # tnet_before_max = trial.suggest_int('tnet_before_max', 1,3)\n",
    "    # tnet_before = []\n",
    "    # for i in range(tnet_before_max):\n",
    "    #     tnet_before.append(trial.suggest_int('tnet_beforemax_layer_'+str(i), 8,128))\n",
    "    # tnet_after = []\n",
    "    # tnet_after_max = trial.suggest_int('tnet_after_max', 1,3)\n",
    "    # for i in range(tnet_after_max):\n",
    "    #     tnet_after.append(trial.suggest_int('tnet_aftermax_layer_'+str(i), 8,128))\n",
    "    # tnet_dense = []\n",
    "    # tnet_dense_layers = trial.suggest_int('tnet_dense_layers', 1,3)\n",
    "    # for i in range(tnet_dense_layers):\n",
    "    #     tnet_dense.append(trial.suggest_int('tnet_dense_layer_'+str(i), 16,256))\n",
    "    tnet_before = [32]\n",
    "    tnet_after = [128]\n",
    "    tnet_dense = [128, 128]\n",
    "    tnet_shape = [tnet_before, tnet_after, tnet_dense]\n",
    "\n",
    "    # gc_layers = trial.suggest_int('gc_layers', 1,3) #1\n",
    "    conv_gnns = []\n",
    "    # for _ in range(gc_layers):\n",
    "    #     before_edge_gcl = trial.suggest_int('before_edge_gcl', 1,3) #2\n",
    "    #     after_edge_gcl = trial.suggest_int('after_edge_gcl', 1,3) #2\n",
    "\n",
    "    #     bfr_edge = []\n",
    "    #     for i in range(before_edge_gcl):\n",
    "    #         bfr_edge.append(trial.suggest_int('before_edge_gcl_'+str(i), 8,128))\n",
    "            \n",
    "    #     aft_edge = []\n",
    "    #     for i in range(after_edge_gcl):\n",
    "    #         aft_edge.append(trial.suggest_int('after_edge_gcl_'+str(i), 8,128))\n",
    "            \n",
    "    #     conv_gnns.append([bfr_edge, aft_edge])\n",
    "    bfr_edge = [32, 64]\n",
    "    aft_edge = [128]\n",
    "    conv_gnns.append([bfr_edge, aft_edge])\n",
    "    # dense_layers = trial.suggest_int('dense_layers', 1,3)\n",
    "    # dense_gnn = []\n",
    "    # for i in range(dense_layers):\n",
    "    #     dense_gnn.append(trial.suggest_int('dense_layer_'+str(i), 16,256))\n",
    "    dense_gnn = [256, 128]\n",
    "        \n",
    "    lr = trial.suggest_float('lr', 0.0001, 0.1)\n",
    "    steps_per_epoch=trial.suggest_int('steps_per_epoch', 15,100)\n",
    "\n",
    "    validation_steps=25     # Static\n",
    "    num_points = 200        # Static\n",
    "    num_dims = 3            # Static\n",
    "    ##############################################################################\n",
    "\n",
    "    inputs = keras.Input(shape=(None, num_dims))\n",
    "    \n",
    "    outputs = build_model(inputs,\n",
    "                    num_points, num_dims, k,\n",
    "                    tnet_shape,\n",
    "                    conv_gnns,\n",
    "                    dense_gnn\n",
    "                )\n",
    "    model = keras.Model(inputs=[inputs], outputs=outputs, name=\"gnn_pointnet\")\n",
    "\n",
    "    opt_pi = tf.optimizers.Adam(learning_rate =  lr )\n",
    "    # opt_pi = tf.optimizers.RMSprop(learning_rate =  lr )\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=opt_pi, metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    # model.compile(loss=tf.nn.sparse_softmax_cross_entropy_with_logits , optimizer=opt_pi, metrics=['accuracy'])\n",
    "    # model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=opt_pi, metrics=['accuracy'])\n",
    "    # model.compile(loss=custom_loss, optimizer=opt_pi, metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_path = experiments_path+test_name+\"/cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, \n",
    "        verbose=0, \n",
    "        save_weights_only=False,\n",
    "        save_freq=10*batch_size)\n",
    "        \n",
    "    latest = tf.train.latest_checkpoint(experiments_path+test_name+\"/\")\n",
    "    if latest:\n",
    "        model.load_weights(latest)\n",
    "        latest_ep = int(latest.split('/')[-1].split('-')[-1].split('.')[0])\n",
    "        print(\" Model loaded correctly:\", latest, \" - Epoch \", latest_ep)\n",
    "    else:\n",
    "        print(\" The model could not be loaded properly: \", latest)\n",
    "        model.save(checkpoint_path.format(epoch=0))\n",
    "        latest_ep = 0\n",
    "\n",
    "    # Use CPU as default due to GPU's memory issues\n",
    "    with tf.device('/CPU:0'):\n",
    "        history = model.fit(\n",
    "            train_x, \n",
    "            train_y, \n",
    "            \n",
    "            initial_epoch=latest_ep,\n",
    "            batch_size=batch_size, \n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=validation_steps,\n",
    "\n",
    "            validation_split=0.3,\n",
    "            epochs=150 - latest_ep, # Train for 150 epochs to find the configuration that can later be trained for more epochs.\n",
    "            shuffle=True,\n",
    "            callbacks=[ValAccThresh_CB(thresh=0.9), cp_callback, history_logger],\n",
    "            use_multiprocessing=False,\n",
    "            workers=8,\n",
    "        )\n",
    "    return np.mean(history.history['val_sparse_categorical_accuracy'][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually train\n",
    "storage = optuna.storages.RDBStorage(url=\"sqlite:///gnn_fixed.db\", engine_kwargs={\"connect_args\": {\"timeout\": 5}})\n",
    "study = optuna.create_study(study_name=\"gnn_denoising_fixed\", storage=storage, load_if_exists=True, direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Configurations Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 17:19:57.024135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-03 17:19:57.025080: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-04-03 17:19:57.025274: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-04-03 17:19:57.025433: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-04-03 17:19:57.025588: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-04-03 17:19:57.025740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-04-03 17:19:57.025887: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-04-03 17:19:57.026034: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-04-03 17:19:57.026183: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-04-03 17:19:57.026202: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-04-03 17:19:57.026618: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"global_max_pooling1d_1\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, None, 16, 512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb Cella 9\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m conv_gnns \u001b[39m=\u001b[39m [[[\u001b[39m128\u001b[39m], [\u001b[39m128\u001b[39m,\u001b[39m32\u001b[39m]]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m dense_gnn \u001b[39m=\u001b[39m [\u001b[39m64\u001b[39m,\u001b[39m64\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m outputs \u001b[39m=\u001b[39m build_model(inputs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m                     num_points, num_dims, k,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                     tnet_shape,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                     conv_gnns,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                     dense_gnn\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                 )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39m[inputs], outputs\u001b[39m=\u001b[39moutputs, name\u001b[39m=\u001b[39mtest_name\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnet\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m opt_pi \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mlr)\n",
      "\u001b[1;32m/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb Cella 9\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(inputs, num_points, num_dims, k, tnet_shape, conv_gnns, dense_gnn, classes)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     x \u001b[39m=\u001b[39m gnn_conv2d(x, gc_filt, [\u001b[39m1\u001b[39m], bn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# x = tf.reduce_max(x, axis=-2, keepdims=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[0][-1])))( [x, tf.shape(inputs)[0]] )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# edge_feats = lambda_edge_feature(x, nn_idxs, k, num_points, l[0][-1])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# x = edge_feats\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m feat_T \u001b[39m=\u001b[39m gnn_tnet(x, x\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], tnet_shape[\u001b[39m1\u001b[39;49m], bn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# print(\" [main] tnet SHAPES: \", x.shape, feat_T.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mDot(axes\u001b[39m=\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))([x, feat_T])\n",
      "\u001b[1;32m/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb Cella 9\u001b[0m in \u001b[0;36mgnn_tnet\u001b[0;34m(inputs, num_dims, tnet_shapes, bn)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39mfor\u001b[39;00m filt \u001b[39min\u001b[39;00m tnet_shapes[\u001b[39m1\u001b[39m]:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     x \u001b[39m=\u001b[39m gnn_conv2d(inputs, filters\u001b[39m=\u001b[39mfilt, kernel_size\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m], bn\u001b[39m=\u001b[39mbn)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mGlobalMaxPooling1D(keepdims\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39m# print(\" [tnet] global maxpool 2d shape: \", x.shape)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X11sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mLambda(\u001b[39mlambda\u001b[39;00m y: tf\u001b[39m.\u001b[39mreshape(y[\u001b[39m0\u001b[39m], (y[\u001b[39m1\u001b[39m], y[\u001b[39m2\u001b[39m])))( [x, batch_size, x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]] )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/input_spec.py:213\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    211\u001b[0m   ndim \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mrank\n\u001b[1;32m    212\u001b[0m   \u001b[39mif\u001b[39;00m ndim \u001b[39m!=\u001b[39m spec\u001b[39m.\u001b[39mndim:\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    214\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    215\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mexpected ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m, found ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    216\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmax_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m   ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"global_max_pooling1d_1\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, None, 16, 512)"
     ]
    }
   ],
   "source": [
    "test_name = \"manual_test_double_tnet_07\"\n",
    "\n",
    "k=16\n",
    "batch_size=32\n",
    "steps_per_epoch=32\n",
    "validation_steps=25\n",
    "lr = 0.0001\n",
    "\n",
    "num_points=200\n",
    "num_dims=3\n",
    "\n",
    "checkpoint_path = experiments_path+test_name+\"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=0,\n",
    "    save_weights_only=False,\n",
    "    save_freq=10*steps_per_epoch)\n",
    "\n",
    "inputs = keras.Input(shape=(None, 3))\n",
    "\n",
    "tnet_shape = [[[32, 64], [512], [256,128]], [[128], [512], [256,128]]]\n",
    "conv_gnns = [[[128], [128,32]]]\n",
    "dense_gnn = [64,64]\n",
    "\n",
    "outputs = build_model(inputs,\n",
    "                    num_points, num_dims, k,\n",
    "                    tnet_shape,\n",
    "                    conv_gnns,\n",
    "                    dense_gnn\n",
    "                )\n",
    "\n",
    "model = keras.Model(inputs=[inputs], outputs=outputs, name=test_name+\"net\")\n",
    "\n",
    "\n",
    "opt_pi = tf.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=opt_pi, metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# Try to load the model. If it does not exist, create it.\n",
    "# latest = tf.train.latest_checkpoint(experiments_path+test_name+\"/\")\n",
    "latest = sorted([ f.path for f in os.scandir(experiments_path+test_name) if f.is_dir() ])[-1] \\\n",
    "    if os.path.isdir(experiments_path+test_name) else None\n",
    "\n",
    "if latest:\n",
    "    # https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "    # model.load(latest)\n",
    "    model = tf.keras.models.load_model(latest)\n",
    "    latest_ep = int(latest.split('/')[-1].split('-')[-1].split('.')[0])\n",
    "    print(\" Model loaded correctly:\", latest, \" - Epoch \", latest_ep)\n",
    "else:\n",
    "    print(\" The model at \", experiments_path+test_name+\"/\", \"could not be loaded properly: \", latest)\n",
    "    model.save(checkpoint_path.format(epoch=0))\n",
    "    latest_ep = 0\n",
    "\n",
    "# This grants no overwriting of the history file\n",
    "filename=experiments_path+test_name+\"/history\"+str(latest_ep)+\".csv\"\n",
    "history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "\n",
    "# Use CPU as default due to GPU's memory issues\n",
    "with tf.device('/CPU:0'):\n",
    "    history = model.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        \n",
    "        initial_epoch=latest_ep,\n",
    "        batch_size=batch_size, \n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "\n",
    "        validation_split=0.3,\n",
    "        epochs=5000,\n",
    "        shuffle=True,\n",
    "        callbacks=[ValAccThresh_CB(thresh=0.85, experiments_path=experiments_path, test_name=test_name), cp_callback, history_logger],\n",
    "        use_multiprocessing=False,\n",
    "        workers=8,\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
