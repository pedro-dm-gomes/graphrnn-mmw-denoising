{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph NN for MMWave Sensor filtering\n",
    "The idea is to train a classifier to distinguish between fake points and actual ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 11:00:13.408983: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-07-20 11:00:13.409032: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data shape:  (7263268, 7)\n",
      "Points per cloud:  200\n",
      "Skipping because:  (68, 7) 7263200 7263400\n",
      "Skipped 1 point clouds\n",
      " Data shape:  (1148223, 7)\n",
      "Points per cloud:  200\n",
      "Skipping because:  (23, 7) 1148200 1148400\n",
      "Skipped 1 point clouds\n",
      " Data shape:  (3411821, 7)\n",
      "Points per cloud:  200\n",
      "Skipping because:  (21, 7) 3411800 3412000\n",
      "Skipped 1 point clouds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Requirements:\n",
    "     * TF:      2.7.0\n",
    "     * Keras:   2.7.0\n",
    "     * numpy:   1.22.3\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "'''\n",
    "    Setting Up matplotlib for paper compliant figures\n",
    "    (this should avoid problems when compiling latex stuff)\n",
    "'''\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Where datasets are stored\n",
    "run_path = \"data/\"\n",
    "experiments_path = \"experiments/\"\n",
    "\n",
    "from utils import load_train_test_data, get_data_and_label2\n",
    "\n",
    "valid_filter = [9, 69, 73, 3, 55]\n",
    "train_filter=[4, 5, 7, 8, 49, 51, 54, 58, 59, 62, 63, 65, 66, 68, 71, 72, 75, 76, 77, 79, 81, 82, 84, 86, 87, 89, 90, 93, 95, 97, 98]\n",
    "test_filter=[53, 67, 74, 80, 88, 96, 61, 6, 64, 92, 70, 85, 50, 56, 57]\n",
    "\n",
    "\n",
    "# Original separation\n",
    "# train_filter=[4,6,8,9,50,51,53,54,56,58,59,61,62,63,65,66,67,69]\n",
    "# test_filter=[3,7,49,55,57,64,68,70]\n",
    "\n",
    "# dataset = load_datasets(run_path)\n",
    "train, valid, test = load_train_test_data(run_path, train_filter=train_filter, valid_filter=valid_filter, test_filter=test_filter)\n",
    "\n",
    "# Separate Train and Test data\n",
    "# d_len = int(len(dataset)*0.7)\n",
    "# train, test = dataset[:d_len], dataset[d_len:]\n",
    "\n",
    "\n",
    "# Shuffle point cloud dataset\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(valid)\n",
    "np.random.shuffle(test)\n",
    "\n",
    "# Get X and Y data for training\n",
    "train_x, train_y = get_data_and_label2(train)\n",
    "valid_x, valid_y = get_data_and_label2(valid)\n",
    "test_x, test_y = get_data_and_label2(test)\n",
    "\n",
    "# len(train_x), len(valid_x), len(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Graph CNN \n",
    "#### https://github.com/WangYueFt/dgcnn\n",
    "#### https://stackoverflow.com/questions/37009647/compute-pairwise-distance-in-a-batch-without-replicating-tensor-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gnn_conv2d(inputs,\n",
    "#             filters,\n",
    "#             kernel_size,\n",
    "#             stride=[1],\n",
    "#             padding='SAME',\n",
    "#             use_xavier=True,\n",
    "#             stddev=1e-3,\n",
    "#             activation_fn=tf.nn.relu,\n",
    "#             bn=False):\n",
    "\n",
    "#     x = layers.Conv1D(\n",
    "#         filters, \n",
    "#         kernel_size, \n",
    "#         strides=stride, \n",
    "#         padding=padding,\n",
    "#         activation=activation_fn,\n",
    "#         kernel_initializer='glorot_uniform' if use_xavier else keras.initializers.TruncatedNormal(stddev=stddev),\n",
    "#         bias_initializer='zeros'\n",
    "#     )(inputs)\n",
    "\n",
    "#     if bn: x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(0.4)(x)\n",
    "#     return x\n",
    "\n",
    "# def gnn_dense(inputs,\n",
    "#             units,\n",
    "#             use_xavier=True,\n",
    "#             stddev=1e-3,\n",
    "#             activation_fn=tf.nn.relu,\n",
    "#             bn=False):\n",
    "            \n",
    "#     x = layers.Dense(units,\n",
    "#         activation=activation_fn,\n",
    "#         kernel_initializer='glorot_uniform' if use_xavier else keras.initializers.TruncatedNormal(stddev=stddev),\n",
    "#         bias_initializer='zeros'\n",
    "#     )(inputs)\n",
    "\n",
    "#     if bn: x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(0.4)(x)\n",
    "#     return x\n",
    "\n",
    "# def lambda_get_adj_matr(input):\n",
    "#     pcT = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(input)\n",
    "#     pc_inn = layers.Lambda(lambda x: tf.matmul(x[0], x[1]))( (input, pcT) )\n",
    "#     pc2 = layers.Lambda(lambda x: tf.reduce_sum(tf.square(x), axis=-1, keepdims=True))(input)\n",
    "#     pc2T = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(pc2)\n",
    "#     output = layers.Lambda(lambda x: x[0] + -2 * x[1] + x[2])( (pc2, pc_inn, pc2T) )\n",
    "#     # Uncomment line below to use reciprocal of adj matrix (1/distance)\n",
    "#     # output = layers.Lambda(lambda x: tf.math.reciprocal(x))(output)\n",
    "#     return output\n",
    "\n",
    "# def lambda_knn(adj, k=20):\n",
    "#     x = layers.Lambda(lambda x: tf.math.top_k(-x[0], x[1]))( (adj, k) )\n",
    "#     return x.indices\n",
    "\n",
    "# def lambda_edge_feature(inputs, nn_idxs, k=20, num_points=200, num_dims=3):\n",
    "\n",
    "#     pc_central = inputs\n",
    "#     batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "#     idx_ = layers.Lambda(lambda x: tf.range(x[0]) * x[1])( (batch_size, num_points) )\n",
    "#     idx_ = layers.Lambda(lambda x: tf.reshape(x[0], (x[1], 1, 1)))( (idx_, batch_size) )\n",
    "#     # Adding to list of idxs of k points the points themselves\n",
    "#     pc_temp1 = layers.Lambda(lambda x: x[0]+x[1])( (nn_idxs, idx_) )\n",
    "\n",
    "#     # Flattening of points into a list of coordinates (x,y,z)\n",
    "#     pc_flat = layers.Lambda(lambda x: tf.reshape(x[0], [-1, x[1]]))( (inputs, num_dims) )\n",
    "\n",
    "#     # Collect points from computed idxs\n",
    "#     pc_neighbors = layers.Lambda(lambda x: tf.gather(x[0], x[1]) )( (pc_flat, pc_temp1) )\n",
    "\n",
    "#     # Reshape points into shape (batch, num_points, NEW_AXIS = 1, num_dims)\n",
    "#     pc_central = layers.Lambda(lambda x: tf.expand_dims(x, axis=-2))(pc_central)\n",
    "#     # Points are repeated k-times along new dimension ==> (batch, num_points, k, num_dims)\n",
    "#     pc_central = layers.Lambda(lambda x: tf.tile(x[0], [1, 1, x[1], 1]))( (pc_central, k) )\n",
    "\n",
    "#     pc_temp2 = layers.Lambda(lambda x: tf.subtract(x[0], x[1]))( (pc_neighbors, pc_central) )\n",
    "#     edge_feature = layers.Lambda(lambda x: tf.concat((x[0], x[1]), axis=-1))((pc_central, pc_temp2))\n",
    "#     return edge_feature\n",
    "\n",
    "# class CustomOrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "#     def __init__(self, num_features=3, l2reg=0.0005):\n",
    "#         self.num_features = num_features\n",
    "#         self.l2reg = l2reg\n",
    "#         self.eye = tf.eye(num_features)\n",
    "\n",
    "#     def __call__(self, x):\n",
    "#         x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "#         xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "#         xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "#         return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         return {'num_features': self.num_features, 'l2reg': self.l2reg}\n",
    "\n",
    "# def gnn_tnet(inputs, num_dims, tnet_shapes, bn=False):\n",
    "#     batch_size = tf.shape(inputs)[0]\n",
    "#     for filt in tnet_shapes[0]:\n",
    "#         x = gnn_conv2d(inputs, filters=filt, kernel_size=[1], bn=bn)\n",
    "#     # x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "#     # for filt in tnet_shapes[1]:\n",
    "#     #     x = gnn_conv2d(inputs, filters=filt, kernel_size=[1], bn=bn)\n",
    "#     x = layers.GlobalMaxPooling1D(keepdims=True)(x)\n",
    "#     # print(\" [tnet] global maxpool 2d shape: \", x.shape)\n",
    "#     x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], y[2])))( [x, batch_size, x.shape[-1]] )\n",
    "#     # print(\" [tnet] reshape: \", x.shape)\n",
    "\n",
    "#     for neur in tnet_shapes[2]:\n",
    "#         x = gnn_dense(x, neur, bn)\n",
    "    \n",
    "#     bias = keras.initializers.Constant(np.eye(num_dims).flatten())\n",
    "#     reg = CustomOrthogonalRegularizer(num_features=num_dims)\n",
    "#     x = layers.Dense(\n",
    "#         num_dims * num_dims,\n",
    "#         kernel_initializer=\"zeros\",\n",
    "#         bias_initializer=bias,\n",
    "#         activity_regularizer=reg,\n",
    "#     )(x)\n",
    "#     feat_T = layers.Reshape((num_dims, num_dims), name='tnet_last_reshape'+str(np.round(time.time(), 5)))(x)\n",
    "#     return feat_T\n",
    "\n",
    "# # function to test custom losses\n",
    "# def custom_loss(pred, labels):\n",
    "#     # loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=labels, logits=pred, label_smoothing=0.2)\n",
    "#     # classify_loss = tf.reduce_mean(loss)\n",
    "#     loss = tf.reduce_mean(tf.reduce_sum(tf.math.square(tf.math.subtract(labels, pred)), axis=-1))\n",
    "#     return loss\n",
    "\n",
    "# '''\n",
    "# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "# tf.math.l2_normalize\n",
    "# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "# '''\n",
    "\n",
    "# # Callback to save good models. Threshold is on validation accuracy.\n",
    "# class ValAccThresh_CB(keras.callbacks.Callback):\n",
    "#     def __init__(self, thresh=0.85, experiments_path=\"experiments/\", test_name=\"test\"):\n",
    "#         self.thresh = thresh\n",
    "#         super(keras.callbacks.Callback, self).__init__()\n",
    "#         # best_weights to store the weights at which the minimum loss occurs.\n",
    "#         self.best_weights = None\n",
    "#         self.stopped_epoch = 0\n",
    "#         self.experiments_path = experiments_path\n",
    "#         self.test_name = test_name\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         # self.current_epoch += 1\n",
    "#         val_key = \"\"\n",
    "#         for k in logs.keys():\n",
    "#             if \"val\" in k and \"accuracy\" in k:\n",
    "#                 val_key = k\n",
    "#                 break\n",
    "#         assert logs.get(val_key) != None, print(\" Validation Accuracy not found\", self.thresh, logs.get(val_key), val_key, logs.get(val_key) == None)\n",
    "\n",
    "#         current = logs.get(val_key)\n",
    "#         # current = logs.get(\"val_sparse_categorical_accuracy\")\n",
    "#         # current = logs.get(\"val_accuracy\")\n",
    "#         if current >= self.thresh:\n",
    "#             self.thresh = current\n",
    "#             self.model.save_weights(self.experiments_path+self.test_name+\"/best_weights/cp-\"+str(epoch)+\".ckpt\")\n",
    "#             print(\" New good model saved.\")\n",
    "\n",
    "# Callback to save history for post-processing\n",
    "# filename=experiments_path+test_name+\"/history.csv\"\n",
    "# history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "# test_name = \"test_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gnn_conv2d, gnn_dense, \\\n",
    "    lambda_get_adj_matr, lambda_knn, lambda_edge_feature, \\\n",
    "    CustomOrthogonalRegularizer, gnn_tnet, custom_loss, ValAccThresh_CB\n",
    "\n",
    "def build_model(inputs, \n",
    "        num_points, num_dims, k,\n",
    "        tnet_shape,\n",
    "        conv_gnns,\n",
    "        dense_gnn,\n",
    "        classes=1):\n",
    "    '''\n",
    "        Returns the outputs of the model to be compiled.\n",
    "        ---\n",
    "        Arguments: \n",
    "        * inputs:       Expected (None, 3). instance of tf.Input.\n",
    "        * num_points:   Number of points per point cloud. Default is 200\n",
    "        * num_dims:     Number of dimensions per point. Default is 3 (x, y, z)\n",
    "        * k:            K nearest neighbors\n",
    "        * tnet_shape:   Array of three lists. (each list's length is the number of layers for that section)\n",
    "                        1st is a list of filters for convolutional layers before reduce_max.\n",
    "                        2nd is a list of filters for convolutional layers after reduce_max.\n",
    "                        3rd is a list of neurons for dense layers after max pooling.\n",
    "        * conv_gnns:    list. Each row is composed of two lists.\n",
    "                        1st is a list of filters for convolutional layers before computing edge features.\n",
    "                        2nd is a list of filters for convolutional layers after computing edge features.\n",
    "        * dense_gnn:    list of neuorns for dense layers at the end of the network.\n",
    "        * classes:      number of classes to classify.\n",
    "    '''\n",
    "\n",
    "    # adj = lambda_get_adj_matr(inputs)\n",
    "    # nn_idxs = lambda_knn(adj, k)\n",
    "    # edge_feats = lambda_edge_feature(inputs, nn_idxs, k, num_points, num_dims)\n",
    "    x = inputs\n",
    "    # x = gnn_dense(inputs, 8, bn=False)\n",
    "    feat_T = gnn_tnet(x, num_dims, tnet_shape[0], bn=False)\n",
    "    # print(\" [main] tnet shape: \", inputs.shape, feat_T.shape)\n",
    "    pc_tf = layers.Dot(axes=(-1, -2))([x, feat_T]) # Apply affine transformation to input features\n",
    "    # print(\" [main] dot shape: \", pc_tf.shape)\n",
    "\n",
    "    # adj = lambda_get_adj_matr(pc_tf)\n",
    "    # nn_idxs = lambda_knn(adj, k)\n",
    "    # edge_feats = lambda_edge_feature(pc_tf, nn_idxs, k, num_points, num_dims)\n",
    "    # x = edge_feats\n",
    "    x = pc_tf\n",
    "\n",
    "    for l in conv_gnns:\n",
    "\n",
    "        for gc_filt in l[0]:\n",
    "            x = gnn_conv2d(x, gc_filt, [1], bn=False)\n",
    "        \n",
    "        feat_T = gnn_tnet(x, l[0][-1], tnet_shape[1], bn=False)\n",
    "        # print(\" [main] tnet shape: \", inputs.shape, feat_T.shape)\n",
    "        x = layers.Dot(axes=(-1, -2))([x, feat_T]) # Apply affine transformation to input features\n",
    "        # x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "        # x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[0][-1])))( [x, tf.shape(inputs)[0]] )\n",
    "\n",
    "        # adj = lambda_get_adj_matr(x)\n",
    "        # nn_idxs = lambda_knn(adj, k)\n",
    "        # edge_feats = lambda_edge_feature(x, nn_idxs, k, num_points, l[0][-1])\n",
    "        # x = edge_feats\n",
    "\n",
    "        # feat_T = gnn_tnet(x, x.shape[-1], tnet_shape[1], bn=True)\n",
    "        # print(\" [main] tnet SHAPES: \", x.shape, feat_T.shape)\n",
    "        # x = layers.Dot(axes=(-1, -2))([x, feat_T])\n",
    "        # print(\" [main] DOT SHAPE: \", x.shape)\n",
    "\n",
    "        # adj = lambda_get_adj_matr(x)\n",
    "        # nn_idxs = lambda_knn(adj, k)\n",
    "        # edge_feats = lambda_edge_feature(x, nn_idxs, k, num_points, x.shape[-1])\n",
    "\n",
    "        for gc_filt in l[1]:\n",
    "            x = gnn_conv2d(x, gc_filt, [1], bn=False)\n",
    "        # x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "        # x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[1][-1])))( [x, tf.shape(inputs)[0]] )\n",
    "\n",
    "    # print(tf.shape(x))\n",
    "    # x = layers.GlobalMaxPooling2D(keepdims=True)(x)\n",
    "    # x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], y[2])))( [x, x.shape[0], x.shape[-1]] )\n",
    "    # print(tf.shape(x))\n",
    "    # x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], y[2])))( [x, tf.shape(x)[0], x.shape[-1]] )\n",
    "    # x = layers.GlobalMaxPooling1D()(x)\n",
    "    # print(\" [main] shape: \", x.shape)\n",
    "    for w_ in dense_gnn:\n",
    "        x = gnn_dense(x, w_, bn=False)\n",
    "        # x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    outputs = layers.Dense(classes, activation=\"relu\")(x)\n",
    "    # print(tf.shape(outputs))\n",
    "    return outputs\n",
    "\n",
    "def objective(trial):\n",
    "    test_name = \"fixed_test_\"+str(trial.number)\n",
    "    filename=experiments_path+test_name+\"/history.csv\"\n",
    "    history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "    ############################ HyperParameter Setup ############################\n",
    "    ######################### Check build_model for docs #########################\n",
    "    k = trial.suggest_int('k', 5,75) #30\n",
    "    batch_size = trial.suggest_int('batch_size', 8,128) #16\n",
    "    # tnet_before_max = trial.suggest_int('tnet_before_max', 1,3)\n",
    "    # tnet_before = []\n",
    "    # for i in range(tnet_before_max):\n",
    "    #     tnet_before.append(trial.suggest_int('tnet_beforemax_layer_'+str(i), 8,128))\n",
    "    # tnet_after = []\n",
    "    # tnet_after_max = trial.suggest_int('tnet_after_max', 1,3)\n",
    "    # for i in range(tnet_after_max):\n",
    "    #     tnet_after.append(trial.suggest_int('tnet_aftermax_layer_'+str(i), 8,128))\n",
    "    # tnet_dense = []\n",
    "    # tnet_dense_layers = trial.suggest_int('tnet_dense_layers', 1,3)\n",
    "    # for i in range(tnet_dense_layers):\n",
    "    #     tnet_dense.append(trial.suggest_int('tnet_dense_layer_'+str(i), 16,256))\n",
    "    tnet_before = [32]\n",
    "    tnet_after = [128]\n",
    "    tnet_dense = [128, 128]\n",
    "    tnet_shape = [tnet_before, tnet_after, tnet_dense]\n",
    "\n",
    "    # gc_layers = trial.suggest_int('gc_layers', 1,3) #1\n",
    "    conv_gnns = []\n",
    "    # for _ in range(gc_layers):\n",
    "    #     before_edge_gcl = trial.suggest_int('before_edge_gcl', 1,3) #2\n",
    "    #     after_edge_gcl = trial.suggest_int('after_edge_gcl', 1,3) #2\n",
    "\n",
    "    #     bfr_edge = []\n",
    "    #     for i in range(before_edge_gcl):\n",
    "    #         bfr_edge.append(trial.suggest_int('before_edge_gcl_'+str(i), 8,128))\n",
    "            \n",
    "    #     aft_edge = []\n",
    "    #     for i in range(after_edge_gcl):\n",
    "    #         aft_edge.append(trial.suggest_int('after_edge_gcl_'+str(i), 8,128))\n",
    "            \n",
    "    #     conv_gnns.append([bfr_edge, aft_edge])\n",
    "    bfr_edge = [32, 64]\n",
    "    aft_edge = [128]\n",
    "    conv_gnns.append([bfr_edge, aft_edge])\n",
    "    # dense_layers = trial.suggest_int('dense_layers', 1,3)\n",
    "    # dense_gnn = []\n",
    "    # for i in range(dense_layers):\n",
    "    #     dense_gnn.append(trial.suggest_int('dense_layer_'+str(i), 16,256))\n",
    "    dense_gnn = [256, 128]\n",
    "        \n",
    "    lr = trial.suggest_float('lr', 0.0001, 0.1)\n",
    "    steps_per_epoch=trial.suggest_int('steps_per_epoch', 15,100)\n",
    "\n",
    "    validation_steps=25     # Static\n",
    "    num_points = 200        # Static\n",
    "    num_dims = 3            # Static\n",
    "    ##############################################################################\n",
    "\n",
    "    inputs = keras.Input(shape=(None, num_dims))\n",
    "    \n",
    "    outputs = build_model(inputs,\n",
    "                    num_points, num_dims, k,\n",
    "                    tnet_shape,\n",
    "                    conv_gnns,\n",
    "                    dense_gnn\n",
    "                )\n",
    "    model = keras.Model(inputs=[inputs], outputs=outputs, name=\"gnn_pointnet\")\n",
    "\n",
    "    opt_pi = tf.optimizers.Adam(learning_rate =  lr )\n",
    "    # opt_pi = tf.optimizers.RMSprop(learning_rate =  lr )\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=opt_pi, metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    # model.compile(loss=tf.nn.sparse_softmax_cross_entropy_with_logits , optimizer=opt_pi, metrics=['accuracy'])\n",
    "    # model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=opt_pi, metrics=['accuracy'])\n",
    "    # model.compile(loss=custom_loss, optimizer=opt_pi, metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_path = experiments_path+test_name+\"/cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, \n",
    "        verbose=0, \n",
    "        save_weights_only=False,\n",
    "        save_freq=10*batch_size)\n",
    "        \n",
    "    latest = tf.train.latest_checkpoint(experiments_path+test_name+\"/\")\n",
    "    if latest:\n",
    "        model.load_weights(latest)\n",
    "        latest_ep = int(latest.split('/')[-1].split('-')[-1].split('.')[0])\n",
    "        print(\" Model loaded correctly:\", latest, \" - Epoch \", latest_ep)\n",
    "    else:\n",
    "        print(\" The model could not be loaded properly: \", latest)\n",
    "        model.save(checkpoint_path.format(epoch=0))\n",
    "        latest_ep = 0\n",
    "\n",
    "    # Use CPU as default due to GPU's memory issues\n",
    "    with tf.device('/CPU:0'):\n",
    "        history = model.fit(\n",
    "            train_x, \n",
    "            train_y, \n",
    "            \n",
    "            initial_epoch=latest_ep,\n",
    "            batch_size=batch_size, \n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=validation_steps,\n",
    "\n",
    "            validation_split=0.3,\n",
    "            epochs=150 - latest_ep, # Train for 150 epochs to find the configuration that can later be trained for more epochs.\n",
    "            shuffle=True,\n",
    "            callbacks=[ValAccThresh_CB(thresh=0.9), cp_callback, history_logger],\n",
    "            use_multiprocessing=False,\n",
    "            workers=8,\n",
    "        )\n",
    "    return np.mean(history.history['val_sparse_categorical_accuracy'][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually train\n",
    "storage = optuna.storages.RDBStorage(url=\"sqlite:///gnn_fixed.db\", engine_kwargs={\"connect_args\": {\"timeout\": 5}})\n",
    "study = optuna.create_study(study_name=\"gnn_denoising_fixed\", storage=storage, load_if_exists=True, direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Configurations Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 11:01:37.094357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-20 11:01:37.095613: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-07-20 11:01:37.095885: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-07-20 11:01:37.096070: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-07-20 11:01:37.116132: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-07-20 11:01:37.116426: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-07-20 11:01:37.116648: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-07-20 11:01:37.116676: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-07-20 11:01:37.117325: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model loaded correctly: experiments/manual_test_regress_relu_01/cp-0130.ckpt  - Epoch  130\n",
      "Epoch 131/3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 11:01:43.024481: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1073741824 exceeds 10% of free system memory.\n",
      "2023-07-20 11:01:43.923559: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1073741824 exceeds 10% of free system memory.\n",
      "2023-07-20 11:01:44.609172: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1073741824 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/32 [..............................] - ETA: 3:28 - loss: 417.9144 - custom_metric: 0.6603"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 11:01:46.986771: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1073741824 exceeds 10% of free system memory.\n",
      "2023-07-20 11:01:47.755746: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1073741824 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 135s 4s/step - loss: 461.5260 - custom_metric: 0.6505 - val_loss: 416.8575 - val_custom_metric: 0.6323\n",
      "Epoch 132/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 447.4399 - custom_metric: 0.6455 - val_loss: 398.3902 - val_custom_metric: 0.6529\n",
      "Epoch 133/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 442.9241 - custom_metric: 0.6515 - val_loss: 383.2748 - val_custom_metric: 0.6387\n",
      "Epoch 134/3000\n",
      "32/32 [==============================] - 128s 4s/step - loss: 445.5448 - custom_metric: 0.6458 - val_loss: 381.6011 - val_custom_metric: 0.6326\n",
      "Epoch 135/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 435.6294 - custom_metric: 0.6463 - val_loss: 364.2783 - val_custom_metric: 0.6587\n",
      "Epoch 136/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 433.3120 - custom_metric: 0.6545 - val_loss: 399.2405 - val_custom_metric: 0.6479\n",
      "Epoch 137/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 426.8655 - custom_metric: 0.6510 - val_loss: 346.2874 - val_custom_metric: 0.6612\n",
      "Epoch 138/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 451.8411 - custom_metric: 0.6550 - val_loss: 395.4380 - val_custom_metric: 0.6493\n",
      "Epoch 139/3000\n",
      "32/32 [==============================] - 128s 4s/step - loss: 451.8686 - custom_metric: 0.6564 - val_loss: 372.8619 - val_custom_metric: 0.6619\n",
      "Epoch 140/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 436.5823 - custom_metric: 0.6592"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 11:22:55.314611: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0140.ckpt/assets\n",
      "32/32 [==============================] - 135s 4s/step - loss: 436.1883 - custom_metric: 0.6599 - val_loss: 395.5173 - val_custom_metric: 0.6326\n",
      "Epoch 141/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 423.4409 - custom_metric: 0.6581 - val_loss: 380.8686 - val_custom_metric: 0.6579\n",
      "Epoch 142/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 447.1883 - custom_metric: 0.6554 - val_loss: 396.1052 - val_custom_metric: 0.6581\n",
      "Epoch 143/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 451.9998 - custom_metric: 0.6504 - val_loss: 356.3167 - val_custom_metric: 0.6503\n",
      "Epoch 144/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 432.3571 - custom_metric: 0.6474 - val_loss: 383.8533 - val_custom_metric: 0.6450\n",
      "Epoch 145/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 448.8497 - custom_metric: 0.6538 - val_loss: 370.0607 - val_custom_metric: 0.6571\n",
      "Epoch 146/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 423.1985 - custom_metric: 0.6531 - val_loss: 391.2249 - val_custom_metric: 0.6458\n",
      "Epoch 147/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 448.5892 - custom_metric: 0.6546 - val_loss: 374.6182 - val_custom_metric: 0.6560\n",
      "Epoch 148/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 412.0238 - custom_metric: 0.6565 - val_loss: 369.7690 - val_custom_metric: 0.6592\n",
      "Epoch 149/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 434.7003 - custom_metric: 0.6542 - val_loss: 365.9547 - val_custom_metric: 0.6590\n",
      "Epoch 150/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 438.4104 - custom_metric: 0.6564INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0150.ckpt/assets\n",
      "32/32 [==============================] - 136s 4s/step - loss: 434.6795 - custom_metric: 0.6567 - val_loss: 363.7762 - val_custom_metric: 0.6495\n",
      "Epoch 151/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 432.1343 - custom_metric: 0.6538 - val_loss: 370.3903 - val_custom_metric: 0.6279\n",
      "Epoch 152/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 432.7979 - custom_metric: 0.6481 - val_loss: 361.8127 - val_custom_metric: 0.6667\n",
      "Epoch 153/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 438.4095 - custom_metric: 0.6528 - val_loss: 412.6808 - val_custom_metric: 0.6574\n",
      "Epoch 154/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 425.3645 - custom_metric: 0.6607 - val_loss: 363.9182 - val_custom_metric: 0.6322\n",
      "Epoch 155/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 433.8812 - custom_metric: 0.6511 - val_loss: 399.9070 - val_custom_metric: 0.6666\n",
      "Epoch 156/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 438.4045 - custom_metric: 0.6585 - val_loss: 393.9064 - val_custom_metric: 0.6353\n",
      "Epoch 157/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 437.5746 - custom_metric: 0.6538 - val_loss: 357.8394 - val_custom_metric: 0.6334\n",
      "Epoch 158/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 416.4862 - custom_metric: 0.6536 - val_loss: 374.6469 - val_custom_metric: 0.6634\n",
      "Epoch 159/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 444.0370 - custom_metric: 0.6557 - val_loss: 359.0472 - val_custom_metric: 0.6522\n",
      "Epoch 160/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 427.1963 - custom_metric: 0.6569INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0160.ckpt/assets\n",
      "32/32 [==============================] - 134s 4s/step - loss: 430.1474 - custom_metric: 0.6570 - val_loss: 412.4268 - val_custom_metric: 0.6358\n",
      "Epoch 161/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 438.4497 - custom_metric: 0.6559 - val_loss: 387.0301 - val_custom_metric: 0.6559\n",
      "Epoch 162/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 432.1090 - custom_metric: 0.6553 - val_loss: 360.4732 - val_custom_metric: 0.6650\n",
      "Epoch 163/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 431.0136 - custom_metric: 0.6581 - val_loss: 386.3187 - val_custom_metric: 0.6462\n",
      "Epoch 164/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 414.7368 - custom_metric: 0.6544 - val_loss: 360.8824 - val_custom_metric: 0.6642\n",
      "Epoch 165/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 433.0676 - custom_metric: 0.6566 - val_loss: 374.3752 - val_custom_metric: 0.6482\n",
      "Epoch 166/3000\n",
      "32/32 [==============================] - 128s 4s/step - loss: 419.3631 - custom_metric: 0.6545 - val_loss: 377.9308 - val_custom_metric: 0.6705\n",
      "Epoch 167/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 405.9630 - custom_metric: 0.6624 - val_loss: 364.9945 - val_custom_metric: 0.6509\n",
      "Epoch 168/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 398.7959 - custom_metric: 0.6658 - val_loss: 367.2895 - val_custom_metric: 0.6580\n",
      "Epoch 169/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 415.4698 - custom_metric: 0.6681 - val_loss: 387.5006 - val_custom_metric: 0.6750\n",
      "Epoch 170/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 441.3252 - custom_metric: 0.6599INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0170.ckpt/assets\n",
      "32/32 [==============================] - 135s 4s/step - loss: 439.0140 - custom_metric: 0.6605 - val_loss: 360.0173 - val_custom_metric: 0.6376\n",
      "Epoch 171/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 422.5018 - custom_metric: 0.6515 - val_loss: 423.2448 - val_custom_metric: 0.6389\n",
      "Epoch 172/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 406.2534 - custom_metric: 0.6573 - val_loss: 378.5131 - val_custom_metric: 0.6563\n",
      "Epoch 173/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 437.5691 - custom_metric: 0.6569 - val_loss: 386.9265 - val_custom_metric: 0.6744\n",
      "Epoch 174/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 433.6917 - custom_metric: 0.6600 - val_loss: 417.1082 - val_custom_metric: 0.6316\n",
      "Epoch 175/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 426.4237 - custom_metric: 0.6597 - val_loss: 392.4378 - val_custom_metric: 0.6641\n",
      "Epoch 176/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 409.0294 - custom_metric: 0.6580 - val_loss: 375.3900 - val_custom_metric: 0.6704\n",
      "Epoch 177/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 430.9521 - custom_metric: 0.6652 - val_loss: 390.6682 - val_custom_metric: 0.6620\n",
      "Epoch 178/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 406.5758 - custom_metric: 0.6657 - val_loss: 360.7024 - val_custom_metric: 0.6627\n",
      "Epoch 179/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 416.5267 - custom_metric: 0.6528 - val_loss: 504.1677 - val_custom_metric: 0.6460\n",
      "Epoch 180/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 436.6367 - custom_metric: 0.6541INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0180.ckpt/assets\n",
      "32/32 [==============================] - 135s 4s/step - loss: 434.6721 - custom_metric: 0.6533 - val_loss: 392.3588 - val_custom_metric: 0.6172\n",
      "Epoch 181/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 410.7594 - custom_metric: 0.6653 - val_loss: 396.8666 - val_custom_metric: 0.6382\n",
      "Epoch 182/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 415.4317 - custom_metric: 0.6599 - val_loss: 378.0571 - val_custom_metric: 0.6378\n",
      "Epoch 183/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 410.0892 - custom_metric: 0.6614 - val_loss: 367.8935 - val_custom_metric: 0.6466\n",
      "Epoch 184/3000\n",
      "32/32 [==============================] - 128s 4s/step - loss: 411.2382 - custom_metric: 0.6592 - val_loss: 331.9987 - val_custom_metric: 0.6790\n",
      "Epoch 185/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 425.5015 - custom_metric: 0.6660 - val_loss: 350.4812 - val_custom_metric: 0.6596\n",
      "Epoch 186/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 411.2979 - custom_metric: 0.6628 - val_loss: 387.6068 - val_custom_metric: 0.6587\n",
      "Epoch 187/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 406.6493 - custom_metric: 0.6603 - val_loss: 344.0711 - val_custom_metric: 0.6864\n",
      "Epoch 188/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 407.4852 - custom_metric: 0.6647 - val_loss: 394.9637 - val_custom_metric: 0.6685\n",
      "Epoch 189/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 416.9528 - custom_metric: 0.6672 - val_loss: 377.7497 - val_custom_metric: 0.6647\n",
      "Epoch 190/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 417.8045 - custom_metric: 0.6656INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0190.ckpt/assets\n",
      "32/32 [==============================] - 134s 4s/step - loss: 418.0051 - custom_metric: 0.6659 - val_loss: 380.3945 - val_custom_metric: 0.6616\n",
      "Epoch 191/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 385.5019 - custom_metric: 0.6638 - val_loss: 343.1356 - val_custom_metric: 0.6553\n",
      "Epoch 192/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 420.2102 - custom_metric: 0.6715 - val_loss: 340.6617 - val_custom_metric: 0.6551\n",
      "Epoch 193/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 399.1681 - custom_metric: 0.6556 - val_loss: 349.4332 - val_custom_metric: 0.6770\n",
      "Epoch 194/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 413.0959 - custom_metric: 0.6637 - val_loss: 371.2180 - val_custom_metric: 0.6535\n",
      "Epoch 195/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 392.0387 - custom_metric: 0.6679 - val_loss: 382.6873 - val_custom_metric: 0.6616\n",
      "Epoch 196/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 412.8096 - custom_metric: 0.6606 - val_loss: 354.8178 - val_custom_metric: 0.6589\n",
      "Epoch 197/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 419.1961 - custom_metric: 0.6610 - val_loss: 353.0819 - val_custom_metric: 0.6762\n",
      "Epoch 198/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 419.1331 - custom_metric: 0.6685 - val_loss: 411.5201 - val_custom_metric: 0.6889\n",
      "Epoch 199/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 412.7580 - custom_metric: 0.6721 - val_loss: 356.9618 - val_custom_metric: 0.6656\n",
      "Epoch 200/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 402.9875 - custom_metric: 0.6670INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0200.ckpt/assets\n",
      "32/32 [==============================] - 135s 4s/step - loss: 401.1103 - custom_metric: 0.6668 - val_loss: 370.0180 - val_custom_metric: 0.6777\n",
      "Epoch 201/3000\n",
      "32/32 [==============================] - 128s 4s/step - loss: 401.7998 - custom_metric: 0.6714 - val_loss: 400.8625 - val_custom_metric: 0.6696\n",
      "Epoch 202/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 398.8297 - custom_metric: 0.6717 - val_loss: 359.0586 - val_custom_metric: 0.6819\n",
      "Epoch 203/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 392.2440 - custom_metric: 0.6650 - val_loss: 374.5702 - val_custom_metric: 0.6942\n",
      "Epoch 204/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 410.5648 - custom_metric: 0.6644 - val_loss: 476.9194 - val_custom_metric: 0.6598\n",
      "Epoch 205/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 389.8371 - custom_metric: 0.6598 - val_loss: 422.3061 - val_custom_metric: 0.6606\n",
      "Epoch 206/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 392.1707 - custom_metric: 0.6659 - val_loss: 363.7736 - val_custom_metric: 0.6732\n",
      "Epoch 207/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 436.4636 - custom_metric: 0.6575 - val_loss: 393.3266 - val_custom_metric: 0.6885\n",
      "Epoch 208/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 392.0857 - custom_metric: 0.6775 - val_loss: 408.9885 - val_custom_metric: 0.6683\n",
      "Epoch 209/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 403.5126 - custom_metric: 0.6707 - val_loss: 327.2051 - val_custom_metric: 0.6873\n",
      "Epoch 210/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 407.5698 - custom_metric: 0.6695INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0210.ckpt/assets\n",
      "32/32 [==============================] - 134s 4s/step - loss: 406.8346 - custom_metric: 0.6698 - val_loss: 366.3581 - val_custom_metric: 0.6848\n",
      "Epoch 211/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 390.6138 - custom_metric: 0.6742 - val_loss: 378.6931 - val_custom_metric: 0.6658\n",
      "Epoch 212/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 417.2225 - custom_metric: 0.6672 - val_loss: 384.1086 - val_custom_metric: 0.6855\n",
      "Epoch 213/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 380.0254 - custom_metric: 0.6764 - val_loss: 375.8701 - val_custom_metric: 0.6854\n",
      "Epoch 214/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 402.3403 - custom_metric: 0.6727 - val_loss: 333.9066 - val_custom_metric: 0.7012\n",
      "Epoch 215/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 405.6976 - custom_metric: 0.6643 - val_loss: 344.4870 - val_custom_metric: 0.6984\n",
      "Epoch 216/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 402.4739 - custom_metric: 0.6730 - val_loss: 348.1364 - val_custom_metric: 0.6545\n",
      "Epoch 217/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 387.1595 - custom_metric: 0.6699 - val_loss: 388.2026 - val_custom_metric: 0.6765\n",
      "Epoch 218/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 409.9601 - custom_metric: 0.6694 - val_loss: 397.1889 - val_custom_metric: 0.6675\n",
      "Epoch 219/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 386.7487 - custom_metric: 0.6783 - val_loss: 378.8784 - val_custom_metric: 0.6739\n",
      "Epoch 220/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 395.7301 - custom_metric: 0.6719INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0220.ckpt/assets\n",
      "32/32 [==============================] - 136s 4s/step - loss: 394.6508 - custom_metric: 0.6720 - val_loss: 356.8206 - val_custom_metric: 0.6687\n",
      "Epoch 221/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 391.1650 - custom_metric: 0.6720 - val_loss: 384.4315 - val_custom_metric: 0.6969\n",
      "Epoch 222/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 396.9501 - custom_metric: 0.6695 - val_loss: 376.2189 - val_custom_metric: 0.6777\n",
      "Epoch 223/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 386.9410 - custom_metric: 0.6772 - val_loss: 372.7437 - val_custom_metric: 0.6638\n",
      "Epoch 224/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 384.0219 - custom_metric: 0.6761 - val_loss: 356.4310 - val_custom_metric: 0.6889\n",
      "Epoch 225/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 392.2141 - custom_metric: 0.6736 - val_loss: 359.3475 - val_custom_metric: 0.6890\n",
      "Epoch 226/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 408.3573 - custom_metric: 0.6712 - val_loss: 404.5615 - val_custom_metric: 0.6292\n",
      "Epoch 227/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 383.6708 - custom_metric: 0.6780 - val_loss: 374.1221 - val_custom_metric: 0.6913\n",
      "Epoch 228/3000\n",
      "32/32 [==============================] - 128s 4s/step - loss: 399.1420 - custom_metric: 0.6690 - val_loss: 364.9207 - val_custom_metric: 0.6924\n",
      "Epoch 229/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 369.7126 - custom_metric: 0.6759 - val_loss: 373.2088 - val_custom_metric: 0.6820\n",
      "Epoch 230/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 377.9172 - custom_metric: 0.6730INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0230.ckpt/assets\n",
      "32/32 [==============================] - 136s 4s/step - loss: 376.4510 - custom_metric: 0.6741 - val_loss: 380.0127 - val_custom_metric: 0.6875\n",
      "Epoch 231/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 384.0746 - custom_metric: 0.6799 - val_loss: 406.0817 - val_custom_metric: 0.6416\n",
      "Epoch 232/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 408.5881 - custom_metric: 0.6694 - val_loss: 404.2333 - val_custom_metric: 0.6744\n",
      "Epoch 233/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 392.0193 - custom_metric: 0.6715 - val_loss: 428.7007 - val_custom_metric: 0.6543\n",
      "Epoch 234/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 405.3678 - custom_metric: 0.6809 - val_loss: 401.9365 - val_custom_metric: 0.6731\n",
      "Epoch 235/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 383.9917 - custom_metric: 0.6769 - val_loss: 381.0821 - val_custom_metric: 0.6890\n",
      "Epoch 236/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 377.8208 - custom_metric: 0.6791 - val_loss: 345.4427 - val_custom_metric: 0.6911\n",
      "Epoch 237/3000\n",
      "32/32 [==============================] - 128s 4s/step - loss: 381.3749 - custom_metric: 0.6766 - val_loss: 475.6718 - val_custom_metric: 0.6708\n",
      "Epoch 238/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 384.4354 - custom_metric: 0.6789 - val_loss: 360.7580 - val_custom_metric: 0.6741\n",
      "Epoch 239/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 398.3705 - custom_metric: 0.6675 - val_loss: 376.9560 - val_custom_metric: 0.6737\n",
      "Epoch 240/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 379.6557 - custom_metric: 0.6786INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0240.ckpt/assets\n",
      "32/32 [==============================] - 135s 4s/step - loss: 379.3672 - custom_metric: 0.6792 - val_loss: 364.8804 - val_custom_metric: 0.6976\n",
      "Epoch 241/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 387.5460 - custom_metric: 0.6779 - val_loss: 372.5731 - val_custom_metric: 0.6490\n",
      "Epoch 242/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 384.2140 - custom_metric: 0.6743 - val_loss: 373.4436 - val_custom_metric: 0.6907\n",
      "Epoch 243/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 359.4471 - custom_metric: 0.6807 - val_loss: 358.9933 - val_custom_metric: 0.6940\n",
      "Epoch 244/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 385.8376 - custom_metric: 0.6847 - val_loss: 380.2835 - val_custom_metric: 0.6860\n",
      "Epoch 245/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 391.1422 - custom_metric: 0.6757 - val_loss: 359.9294 - val_custom_metric: 0.6717\n",
      "Epoch 246/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 377.9299 - custom_metric: 0.6758 - val_loss: 357.5791 - val_custom_metric: 0.6968\n",
      "Epoch 247/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 396.3448 - custom_metric: 0.6781 - val_loss: 376.5173 - val_custom_metric: 0.6842\n",
      "Epoch 248/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 381.8787 - custom_metric: 0.6808 - val_loss: 340.9526 - val_custom_metric: 0.6977\n",
      "Epoch 249/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 374.9763 - custom_metric: 0.6845 - val_loss: 370.4902 - val_custom_metric: 0.6860\n",
      "Epoch 250/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 381.4002 - custom_metric: 0.6786INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0250.ckpt/assets\n",
      "32/32 [==============================] - 134s 4s/step - loss: 385.0822 - custom_metric: 0.6789 - val_loss: 373.1719 - val_custom_metric: 0.6725\n",
      "Epoch 251/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 368.7368 - custom_metric: 0.6812 - val_loss: 422.3905 - val_custom_metric: 0.6817\n",
      "Epoch 252/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 369.2748 - custom_metric: 0.6814 - val_loss: 329.4809 - val_custom_metric: 0.7089\n",
      "Epoch 253/3000\n",
      "32/32 [==============================] - 132s 4s/step - loss: 387.7700 - custom_metric: 0.6728 - val_loss: 404.7717 - val_custom_metric: 0.6973\n",
      "Epoch 254/3000\n",
      "32/32 [==============================] - 132s 4s/step - loss: 380.3572 - custom_metric: 0.6785 - val_loss: 370.4486 - val_custom_metric: 0.7031\n",
      "Epoch 255/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 355.9033 - custom_metric: 0.6838 - val_loss: 363.7510 - val_custom_metric: 0.6905\n",
      "Epoch 256/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 371.0002 - custom_metric: 0.6777 - val_loss: 338.3635 - val_custom_metric: 0.6922\n",
      "Epoch 257/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 381.5217 - custom_metric: 0.6793 - val_loss: 367.5999 - val_custom_metric: 0.6810\n",
      "Epoch 258/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 364.3850 - custom_metric: 0.6806 - val_loss: 381.9292 - val_custom_metric: 0.6844\n",
      "Epoch 259/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 375.1364 - custom_metric: 0.6912 - val_loss: 415.7775 - val_custom_metric: 0.6846\n",
      "Epoch 260/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 382.8098 - custom_metric: 0.6726INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0260.ckpt/assets\n",
      "32/32 [==============================] - 136s 4s/step - loss: 381.0850 - custom_metric: 0.6729 - val_loss: 359.3474 - val_custom_metric: 0.6901\n",
      "Epoch 261/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 385.3988 - custom_metric: 0.6717 - val_loss: 411.8376 - val_custom_metric: 0.6653\n",
      "Epoch 262/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 391.4613 - custom_metric: 0.6845 - val_loss: 391.4143 - val_custom_metric: 0.6703\n",
      "Epoch 263/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 366.1498 - custom_metric: 0.6878 - val_loss: 374.9203 - val_custom_metric: 0.6786\n",
      "Epoch 264/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 360.8596 - custom_metric: 0.6841 - val_loss: 421.7491 - val_custom_metric: 0.6878\n",
      "Epoch 265/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 367.4254 - custom_metric: 0.6897 - val_loss: 406.3342 - val_custom_metric: 0.6793\n",
      "Epoch 266/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 363.0548 - custom_metric: 0.6904 - val_loss: 373.6114 - val_custom_metric: 0.6896\n",
      "Epoch 267/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 377.6094 - custom_metric: 0.6797 - val_loss: 353.1646 - val_custom_metric: 0.7004\n",
      "Epoch 268/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 392.0164 - custom_metric: 0.6824 - val_loss: 367.0499 - val_custom_metric: 0.6909\n",
      "Epoch 269/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 361.8122 - custom_metric: 0.6782 - val_loss: 343.9880 - val_custom_metric: 0.7015\n",
      "Epoch 270/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 354.9043 - custom_metric: 0.6894INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0270.ckpt/assets\n",
      "32/32 [==============================] - 136s 4s/step - loss: 355.9233 - custom_metric: 0.6889 - val_loss: 402.5269 - val_custom_metric: 0.6941\n",
      "Epoch 271/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 369.4755 - custom_metric: 0.6823 - val_loss: 361.6971 - val_custom_metric: 0.6839\n",
      "Epoch 272/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 370.7619 - custom_metric: 0.6852 - val_loss: 397.9536 - val_custom_metric: 0.6929\n",
      "Epoch 273/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 374.9816 - custom_metric: 0.6905 - val_loss: 450.8127 - val_custom_metric: 0.6662\n",
      "Epoch 274/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 357.0334 - custom_metric: 0.6802 - val_loss: 388.0569 - val_custom_metric: 0.6753\n",
      "Epoch 275/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 364.0392 - custom_metric: 0.6849 - val_loss: 387.2735 - val_custom_metric: 0.6931\n",
      "Epoch 276/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 373.0511 - custom_metric: 0.6868 - val_loss: 416.8917 - val_custom_metric: 0.6905\n",
      "Epoch 277/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 366.9835 - custom_metric: 0.6858 - val_loss: 368.4965 - val_custom_metric: 0.6937\n",
      "Epoch 278/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 356.0183 - custom_metric: 0.6868 - val_loss: 390.0148 - val_custom_metric: 0.6926\n",
      "Epoch 279/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 366.0336 - custom_metric: 0.6889 - val_loss: 362.6748 - val_custom_metric: 0.7012\n",
      "Epoch 280/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 359.3691 - custom_metric: 0.6790INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0280.ckpt/assets\n",
      "32/32 [==============================] - 136s 4s/step - loss: 359.3495 - custom_metric: 0.6798 - val_loss: 370.5537 - val_custom_metric: 0.6782\n",
      "Epoch 281/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 362.8969 - custom_metric: 0.6878 - val_loss: 377.3345 - val_custom_metric: 0.6908\n",
      "Epoch 282/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 346.3795 - custom_metric: 0.6823 - val_loss: 352.6445 - val_custom_metric: 0.6847\n",
      "Epoch 283/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 357.6964 - custom_metric: 0.6938 - val_loss: 383.6717 - val_custom_metric: 0.7057\n",
      "Epoch 284/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 359.6603 - custom_metric: 0.6887 - val_loss: 380.4497 - val_custom_metric: 0.6941\n",
      "Epoch 285/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 359.0330 - custom_metric: 0.6890 - val_loss: 389.4806 - val_custom_metric: 0.6827\n",
      "Epoch 286/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 357.9760 - custom_metric: 0.6922 - val_loss: 367.5891 - val_custom_metric: 0.6884\n",
      "Epoch 287/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 343.6588 - custom_metric: 0.6911 - val_loss: 374.0582 - val_custom_metric: 0.6947\n",
      "Epoch 288/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 357.5472 - custom_metric: 0.6905 - val_loss: 369.0565 - val_custom_metric: 0.7033\n",
      "Epoch 289/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 378.7120 - custom_metric: 0.6815 - val_loss: 355.0779 - val_custom_metric: 0.7011\n",
      "Epoch 290/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 371.5847 - custom_metric: 0.6906INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0290.ckpt/assets\n",
      "32/32 [==============================] - 157s 5s/step - loss: 370.3424 - custom_metric: 0.6914 - val_loss: 397.6650 - val_custom_metric: 0.6990\n",
      "Epoch 291/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 357.0681 - custom_metric: 0.6882 - val_loss: 389.4713 - val_custom_metric: 0.6737\n",
      "Epoch 292/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 367.1962 - custom_metric: 0.6963 - val_loss: 406.7318 - val_custom_metric: 0.6920\n",
      "Epoch 293/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 352.5503 - custom_metric: 0.6908 - val_loss: 394.8432 - val_custom_metric: 0.6763\n",
      "Epoch 294/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 332.5589 - custom_metric: 0.6896 - val_loss: 424.6689 - val_custom_metric: 0.6832\n",
      "Epoch 295/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 356.6674 - custom_metric: 0.6855 - val_loss: 390.0759 - val_custom_metric: 0.7015\n",
      "Epoch 296/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 351.3225 - custom_metric: 0.6855 - val_loss: 457.2311 - val_custom_metric: 0.6865\n",
      "Epoch 297/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 354.5002 - custom_metric: 0.6919 - val_loss: 337.4054 - val_custom_metric: 0.6851\n",
      "Epoch 298/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 361.8512 - custom_metric: 0.6841 - val_loss: 356.5177 - val_custom_metric: 0.6925\n",
      "Epoch 299/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 350.9316 - custom_metric: 0.6942 - val_loss: 348.5281 - val_custom_metric: 0.6863\n",
      "Epoch 300/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 348.5306 - custom_metric: 0.6929INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0300.ckpt/assets\n",
      "32/32 [==============================] - 135s 4s/step - loss: 348.9098 - custom_metric: 0.6936 - val_loss: 341.8948 - val_custom_metric: 0.7033\n",
      "Epoch 301/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 353.5406 - custom_metric: 0.6845 - val_loss: 384.7977 - val_custom_metric: 0.7154\n",
      "Epoch 302/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 342.3065 - custom_metric: 0.6951 - val_loss: 313.1610 - val_custom_metric: 0.7002\n",
      "Epoch 303/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 358.4204 - custom_metric: 0.6933 - val_loss: 404.0371 - val_custom_metric: 0.6974\n",
      "Epoch 304/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 348.7882 - custom_metric: 0.6932 - val_loss: 420.9051 - val_custom_metric: 0.6879\n",
      "Epoch 305/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 352.3045 - custom_metric: 0.6893 - val_loss: 331.3586 - val_custom_metric: 0.7041\n",
      "Epoch 306/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 340.6516 - custom_metric: 0.6912 - val_loss: 397.3932 - val_custom_metric: 0.6971\n",
      "Epoch 307/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 337.8684 - custom_metric: 0.6929 - val_loss: 336.3315 - val_custom_metric: 0.7015\n",
      "Epoch 308/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 351.3620 - custom_metric: 0.6909 - val_loss: 421.8911 - val_custom_metric: 0.6928\n",
      "Epoch 309/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 360.6205 - custom_metric: 0.6944 - val_loss: 324.7772 - val_custom_metric: 0.7103\n",
      "Epoch 310/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 343.5269 - custom_metric: 0.6969INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0310.ckpt/assets\n",
      "32/32 [==============================] - 136s 4s/step - loss: 344.8920 - custom_metric: 0.6967 - val_loss: 344.0198 - val_custom_metric: 0.7184\n",
      "Epoch 311/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 351.9912 - custom_metric: 0.6914 - val_loss: 340.6133 - val_custom_metric: 0.7053\n",
      "Epoch 312/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 327.9342 - custom_metric: 0.6887 - val_loss: 349.4696 - val_custom_metric: 0.6843\n",
      "Epoch 313/3000\n",
      "32/32 [==============================] - 131s 4s/step - loss: 339.2915 - custom_metric: 0.6883 - val_loss: 348.0462 - val_custom_metric: 0.7134\n",
      "Epoch 314/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 349.5085 - custom_metric: 0.6938 - val_loss: 404.7592 - val_custom_metric: 0.6955\n",
      "Epoch 315/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 357.6073 - custom_metric: 0.6856 - val_loss: 407.9934 - val_custom_metric: 0.6941\n",
      "Epoch 316/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 349.7692 - custom_metric: 0.6999 - val_loss: 362.7984 - val_custom_metric: 0.6921\n",
      "Epoch 317/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 328.1000 - custom_metric: 0.6906 - val_loss: 411.6284 - val_custom_metric: 0.6794\n",
      "Epoch 318/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 342.4721 - custom_metric: 0.6975 - val_loss: 354.8543 - val_custom_metric: 0.6992\n",
      "Epoch 319/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 351.1600 - custom_metric: 0.6967 - val_loss: 400.3911 - val_custom_metric: 0.7021\n",
      "Epoch 320/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 339.8263 - custom_metric: 0.6910INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0320.ckpt/assets\n",
      "32/32 [==============================] - 135s 4s/step - loss: 337.9071 - custom_metric: 0.6913 - val_loss: 359.8565 - val_custom_metric: 0.7078\n",
      "Epoch 321/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 347.2305 - custom_metric: 0.6940 - val_loss: 418.7849 - val_custom_metric: 0.7006\n",
      "Epoch 322/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 345.2553 - custom_metric: 0.6971 - val_loss: 374.7365 - val_custom_metric: 0.6805\n",
      "Epoch 323/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 334.2295 - custom_metric: 0.6993 - val_loss: 384.2170 - val_custom_metric: 0.6906\n",
      "Epoch 324/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 348.5619 - custom_metric: 0.6913 - val_loss: 385.3214 - val_custom_metric: 0.7067\n",
      "Epoch 325/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 343.9453 - custom_metric: 0.6907 - val_loss: 363.6543 - val_custom_metric: 0.6818\n",
      "Epoch 326/3000\n",
      "32/32 [==============================] - 128s 4s/step - loss: 326.5989 - custom_metric: 0.6911 - val_loss: 372.0917 - val_custom_metric: 0.6923\n",
      "Epoch 327/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 326.3014 - custom_metric: 0.6975 - val_loss: 342.2009 - val_custom_metric: 0.7025\n",
      "Epoch 328/3000\n",
      "32/32 [==============================] - 130s 4s/step - loss: 330.1249 - custom_metric: 0.7011 - val_loss: 352.5706 - val_custom_metric: 0.7227\n",
      "Epoch 329/3000\n",
      "32/32 [==============================] - 129s 4s/step - loss: 334.3623 - custom_metric: 0.6933 - val_loss: 401.4741 - val_custom_metric: 0.6920\n",
      "Epoch 330/3000\n",
      "31/32 [============================>.] - ETA: 3s - loss: 334.3621 - custom_metric: 0.6929INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0330.ckpt/assets\n",
      "32/32 [==============================] - 135s 4s/step - loss: 333.5130 - custom_metric: 0.6924 - val_loss: 348.8698 - val_custom_metric: 0.7109\n",
      "Epoch 331/3000\n",
      "23/32 [====================>.........] - ETA: 29s - loss: 332.6698 - custom_metric: 0.6959"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb Cella 8\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m# Use CPU as default due to GPU's memory issues\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39m/CPU:0\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m         train_x,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m         train_y,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m         \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m         initial_epoch\u001b[39m=\u001b[39;49mlatest_ep,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m         steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m         validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m         \u001b[39m# validation_split=0.3,\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m         validation_data\u001b[39m=\u001b[39;49m(valid_x, valid_y),\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m3000\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m         shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m         callbacks\u001b[39m=\u001b[39;49m[ValAccThresh_CB(thresh\u001b[39m=\u001b[39;49m\u001b[39m0.85\u001b[39;49m, experiments_path\u001b[39m=\u001b[39;49mexperiments_path, test_name\u001b[39m=\u001b[39;49mtest_name), cp_callback, history_logger],\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m         use_multiprocessing\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m         workers\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1210\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1211\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1212\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1213\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1214\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1215\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1216\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1217\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1218\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    907\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 910\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    912\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    913\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    939\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    940\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    941\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    945\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3127\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   3128\u001b[0m   (graph_function,\n\u001b[1;32m   3129\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   3131\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1955\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1956\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1957\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1958\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1960\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1961\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m     args,\n\u001b[1;32m   1963\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1964\u001b[0m     executing_eagerly)\n\u001b[1;32m   1965\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    597\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    599\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    601\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    602\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    604\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    606\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    607\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    611\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 58\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     59\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class CustomMetric(tf.keras.metrics.Accuracy):\n",
    "\n",
    "  def __init__(self, name='custom_metric', thresh=0.1, **kwargs):\n",
    "    super(CustomMetric, self).__init__(name=name, **kwargs)\n",
    "    self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "    # self.accuracy_comp = tf.keras.metrics.sparse_categorical_crossentropy()\n",
    "    self.thresh = thresh\n",
    "    self.acc = 0.\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    # print(y_true.shape, y_pred.shape)\n",
    "    # y_true = tf.cast((y_true <= self.thresh), tf.bool)\n",
    "    # y_pred = tf.cast((tf.squeeze(y_pred) <= self.thresh), tf.bool)\n",
    "    # y_true = tf.squeeze(tf.cast(y_true == 0, tf.int32))\n",
    "    # y_pred = tf.squeeze(tf.cast(y_pred == 0, tf.int32))\n",
    "    y_true = tf.cast(y_true <= self.thresh, tf.int32)\n",
    "    y_pred = tf.cast(y_pred <= self.thresh, tf.int32)\n",
    "\n",
    "    # print(y_true.shape, y_pred.shape)\n",
    "    super().update_state(y_true, y_pred, sample_weight)\n",
    "    # self.acc = tf.keras.metrics.categorical_accuracy(y_true, y_pred)\n",
    "    # tf.print(\"[CustomMetric/UpdateState]: Acc: \", self.acc)\n",
    "    # values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
    "    # values = tf.cast(values, self.dtype)\n",
    "    # if sample_weight is not None:\n",
    "    #   sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "    #   sample_weight = tf.broadcast_to(sample_weight, values.shape)\n",
    "    #   values = tf.multiply(values, sample_weight)\n",
    "    # self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "#   def result(self):\n",
    "#     # return self.true_positives\n",
    "#     return self.acc\n",
    "  \n",
    "from utils import ValAccThresh_CB\n",
    "\n",
    "test_name = \"manual_test_regress_relu_01\"\n",
    "\n",
    "k=16\n",
    "batch_size=128\n",
    "steps_per_epoch=32\n",
    "validation_steps=25\n",
    "lr = 0.0001\n",
    "\n",
    "num_points=200\n",
    "num_dims=3\n",
    "\n",
    "checkpoint_path = experiments_path+test_name+\"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=0,\n",
    "    save_weights_only=False,\n",
    "    save_freq=10*steps_per_epoch)\n",
    "\n",
    "inputs = keras.Input(shape=(None, 3))\n",
    "\n",
    "tnet_shape = [[[32, 64], [8], [64]], [[512], [8], [256]]]\n",
    "conv_gnns = [[[128], [256, 128]]]\n",
    "dense_gnn = [512,128]\n",
    "\n",
    "outputs = build_model(inputs,\n",
    "                    num_points, num_dims, k,\n",
    "                    tnet_shape,\n",
    "                    conv_gnns,\n",
    "                    dense_gnn\n",
    "                )\n",
    "\n",
    "model = keras.Model(inputs=[inputs], outputs=outputs, name=test_name+\"net\")\n",
    "\n",
    "\n",
    "opt_pi = tf.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.math.square(y_true*100 - y_pred*100), axis=-1)\n",
    "# model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=opt_pi, metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.compile(loss=custom_loss, optimizer=opt_pi, metrics=[CustomMetric()])\n",
    "\n",
    "# Try to load the model. If it does not exist, create it.\n",
    "# latest = tf.train.latest_checkpoint(experiments_path+test_name+\"/\")\n",
    "latest = sorted([ f.path for f in os.scandir(experiments_path+test_name) if f.is_dir() ])[-1] \\\n",
    "    if os.path.isdir(experiments_path+test_name) else None\n",
    "\n",
    "if latest:\n",
    "    # https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "    # model.load(latest)\n",
    "    model = tf.keras.models.load_model(latest, custom_objects={'CustomOrthogonalRegularizer': CustomOrthogonalRegularizer, 'custom_loss': custom_loss, 'CustomMetric': CustomMetric})\n",
    "    latest_ep = int(latest.split('/')[-1].split('-')[-1].split('.')[0])\n",
    "    print(\" Model loaded correctly:\", latest, \" - Epoch \", latest_ep)\n",
    "else:\n",
    "    print(\" The model at \", experiments_path+test_name+\"/\", \"could not be loaded properly: \", latest)\n",
    "    model.save(checkpoint_path.format(epoch=0))\n",
    "    latest_ep = 0\n",
    "\n",
    "# This grants no overwriting of the history file\n",
    "filename=experiments_path+test_name+\"/history\"+str(latest_ep)+\".csv\"\n",
    "history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "\n",
    "# Use CPU as default due to GPU's memory issues\n",
    "with tf.device('/CPU:0'):\n",
    "    history = model.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        \n",
    "        initial_epoch=latest_ep,\n",
    "        batch_size=batch_size, \n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "\n",
    "        # validation_split=0.3,\n",
    "        validation_data=(valid_x, valid_y),\n",
    "        epochs=3000,\n",
    "        shuffle=True,\n",
    "        callbacks=[ValAccThresh_CB(thresh=0.85, experiments_path=experiments_path, test_name=test_name), cp_callback, history_logger],\n",
    "        use_multiprocessing=True,\n",
    "        workers=8,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019624097"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.mean(np.square(model.predict(test_x[0:10]) - test_y[0:10, None, :]), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.losses.SparseCategoricalCrossentropy()(np.int32(test_y == 0), np.squeeze(np.int32(model.predict(test_x) == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "(10, 2) (10, 200)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Can not squeeze dim[1], expected a dimension of 1, got 200 [Op:Squeeze]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb Cella 12\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msqueeze(tf\u001b[39m.\u001b[39mcast(test_y[:\u001b[39m10\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m, tf\u001b[39m.\u001b[39mint32))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(y_pred\u001b[39m.\u001b[39mshape, y_true\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmetrics\u001b[39m.\u001b[39;49msparse_categorical_accuracy(y_true, y_pred,)\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/metrics.py:3601\u001b[0m, in \u001b[0;36msparse_categorical_accuracy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m   3598\u001b[0m \u001b[39m# If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\u001b[39;00m\n\u001b[1;32m   3599\u001b[0m \u001b[39mif\u001b[39;00m (y_true_rank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (y_pred_rank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mlen\u001b[39m(\n\u001b[1;32m   3600\u001b[0m     backend\u001b[39m.\u001b[39mint_shape(y_true)) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(backend\u001b[39m.\u001b[39mint_shape(y_pred))):\n\u001b[0;32m-> 3601\u001b[0m   y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49msqueeze(y_true, [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m   3602\u001b[0m y_pred \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39margmax(y_pred, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   3604\u001b[0m \u001b[39m# If the predicted output and actual output types don't match, force cast them\u001b[39;00m\n\u001b[1;32m   3605\u001b[0m \u001b[39m# to match.\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Can not squeeze dim[1], expected a dimension of 1, got 200 [Op:Squeeze]"
     ]
    }
   ],
   "source": [
    "# np.squeeze(np.int32(model.predict(test_x[:10]) == 0), axis=-1).shape\n",
    "# model.predict(test_x).shape\n",
    "# one hot encode model output\n",
    "y_pred = model.predict(test_x[:10])\n",
    "print(y_pred.shape)\n",
    "y_pred = tf.one_hot(tf.squeeze(tf.cast(y_pred <= 0.1, tf.int32)), 2)\n",
    "y_true = tf.squeeze(tf.cast(test_y[:10] <= 0.1, tf.int32))\n",
    "print(y_pred.shape, y_true.shape)\n",
    "tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred,).numpy()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, May 26 2023, 14:05:08) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
