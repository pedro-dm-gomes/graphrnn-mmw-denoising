{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph NN for MMWave Sensor filtering\n",
    "The idea is to train a classifier to distinguish between fake points and actual ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 10:44:25.978422: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib::/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-03-13 10:44:25.978472: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Requirements:\n",
    "     * TF:      2.7.0\n",
    "     * Keras:   2.7.0\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "'''\n",
    "    Setting Up matplotlib for paper compliant figures\n",
    "    (this should avoid problems when compiling latex stuff)\n",
    "'''\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Where datasets are stored\n",
    "run_path = \"data/\"\n",
    "experiments_path = \"experiments/\"\n",
    "\n",
    "def load_datasets(init_path):\n",
    "    '''\n",
    "        Return dataset as list of pointclouds\n",
    "        ---\n",
    "        Parameters:\n",
    "        * init_path: string; path to folder holding folders of datasets\n",
    "                     (expected folder structure: 'init_path/run_X/labelled_mmw_run_X.json')\n",
    "    '''\n",
    "    data = []\n",
    "    for run in sorted(os.listdir(init_path)):\n",
    "        if \"run_\" in run:\n",
    "            data.extend(json.load(open(init_path+run+\"/labelled_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "    return data\n",
    "\n",
    "def load_train_test_data(init_path, train_filter=[], test_filter=[]):\n",
    "    '''\n",
    "        Return dataset as list of pointclouds\n",
    "        ---\n",
    "        Parameters:\n",
    "        * init_path: string; path to folder holding folders of datasets\n",
    "                     (expected folder structure: 'init_path/run_X/labelled_mmw_run_X.json')\n",
    "    '''\n",
    "    train = []\n",
    "    test = []\n",
    "    for run in sorted(os.listdir(init_path)):\n",
    "        if \"run_\" in run:\n",
    "            if (int(run.split('_')[-1]) in train_filter):\n",
    "                # train.extend(json.load(open(init_path+run+\"/labelled_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "                train.extend(json.load(open(init_path+run+\"/norm_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "            elif (int(run.split('_')[-1]) in test_filter):\n",
    "                test.extend(json.load(open(init_path+run+\"/labelled_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "    return train, test\n",
    "\n",
    "def get_data_and_label(data, points_per_cloud=200):\n",
    "    '''\n",
    "        Return samples for training as np.array, divided as unlabelled data and related labels.\n",
    "        ---\n",
    "        Parameters:\n",
    "        * data: list of point clouds. (Usually loaded with function load_datasets)\n",
    "        * points_per_cloud: number of points to be found in each point cloud. (Default is 200)\n",
    "    '''\n",
    "    d_x, d_y = [], []\n",
    "    for pc in data:\n",
    "        for i in range(0, len(pc), points_per_cloud): \n",
    "            if len(pc[i:i+points_per_cloud]) == points_per_cloud:\n",
    "                t_ = np.array(pc[i:i+points_per_cloud])[:, :3]\n",
    "                d_x.append(t_)\n",
    "                d_y.append(np.array(pc[i:i+points_per_cloud], dtype=np.float32)[:, -1])\n",
    "                # d_y.append(tf.one_hot(np.array(pc[i:i+points_per_cloud], dtype=np.float32)[:, -1], 2)) # One Hotted\n",
    "    d_x, d_y = np.stack(d_x), np.stack(d_y)\n",
    "    return d_x, d_y\n",
    "\n",
    "train_filter=[4,6,8,9,50,51,53,54,56,58,59,61,62,63,65,66,67,69]\n",
    "test_filter=[3,7,49,55,57,64,68,70]\n",
    "\n",
    "# dataset = load_datasets(run_path)\n",
    "train, test = load_train_test_data(run_path, train_filter=train_filter, test_filter=test_filter)\n",
    "\n",
    "# Separate Train and Test data\n",
    "# d_len = int(len(dataset)*0.7)\n",
    "# train, test = dataset[:d_len], dataset[d_len:]\n",
    "\n",
    "\n",
    "# Shuffle point cloud dataset\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(test)\n",
    "\n",
    "# Get X and Y data for training\n",
    "train_x, train_y = get_data_and_label(train)\n",
    "test_x, test_y = get_data_and_label(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Graph CNN \n",
    "#### https://github.com/WangYueFt/dgcnn\n",
    "#### https://stackoverflow.com/questions/37009647/compute-pairwise-distance-in-a-batch-without-replicating-tensor-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_conv2d(inputs,\n",
    "            filters,\n",
    "            kernel_size,\n",
    "            stride=[1, 1],\n",
    "            padding='SAME',\n",
    "            use_xavier=True,\n",
    "            stddev=1e-3,\n",
    "            activation_fn=tf.nn.elu,\n",
    "            bn=False):\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters, \n",
    "        kernel_size, \n",
    "        strides=stride, \n",
    "        padding=padding,\n",
    "        activation=activation_fn,\n",
    "        kernel_initializer='glorot_uniform' if use_xavier else keras.initializers.TruncatedNormal(stddev=stddev),\n",
    "        bias_initializer='zeros'\n",
    "    )(inputs)\n",
    "\n",
    "    if bn: x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    return x\n",
    "\n",
    "def gnn_dense(inputs,\n",
    "            units,\n",
    "            use_xavier=True,\n",
    "            stddev=1e-3,\n",
    "            activation_fn=tf.nn.elu,\n",
    "            bn=False):\n",
    "            \n",
    "    x = layers.Dense(units,\n",
    "        activation=activation_fn,\n",
    "        kernel_initializer='glorot_uniform' if use_xavier else keras.initializers.TruncatedNormal(stddev=stddev),\n",
    "        bias_initializer='zeros'\n",
    "    )(inputs)\n",
    "\n",
    "    if bn: x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    return x\n",
    "\n",
    "def lambda_get_adj_matr(input):\n",
    "    pcT = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(input)\n",
    "    pc_inn = layers.Lambda(lambda x: tf.matmul(x[0], x[1]))( (input, pcT) )\n",
    "    pc2 = layers.Lambda(lambda x: tf.reduce_sum(tf.square(x), axis=-1, keepdims=True))(input)\n",
    "    pc2T = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(pc2)\n",
    "    output = layers.Lambda(lambda x: x[0] + -2 * x[1] + x[2])( (pc2, pc_inn, pc2T) )\n",
    "    # Uncomment line below to use reciprocal of adj matrix (1/distance)\n",
    "    # output = layers.Lambda(lambda x: tf.math.reciprocal(x))(output)\n",
    "    return output\n",
    "\n",
    "def lambda_knn(adj, k=20):\n",
    "    x = layers.Lambda(lambda x: tf.math.top_k(-x[0], x[1]))( (adj, k) )\n",
    "    return x.indices\n",
    "\n",
    "def lambda_edge_feature(inputs, nn_idxs, k=20, num_points=200, num_dims=3):\n",
    "\n",
    "    pc_central = inputs\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    idx_ = layers.Lambda(lambda x: tf.range(x[0]) * x[1])( (batch_size, num_points) )\n",
    "    idx_ = layers.Lambda(lambda x: tf.reshape(x[0], (x[1], 1, 1)))( (idx_, batch_size) )\n",
    "    # Adding to list of idxs of k points the points themselves\n",
    "    pc_temp1 = layers.Lambda(lambda x: x[0]+x[1])( (nn_idxs, idx_) )\n",
    "\n",
    "    # Flattening of points into a list of coordinates (x,y,z)\n",
    "    pc_flat = layers.Lambda(lambda x: tf.reshape(x[0], [-1, x[1]]))( (inputs, num_dims) )\n",
    "\n",
    "    # Collect points from computed idxs\n",
    "    pc_neighbors = layers.Lambda(lambda x: tf.gather(x[0], x[1]) )( (pc_flat, pc_temp1) )\n",
    "\n",
    "    # Reshape points into shape (batch, num_points, NEW_AXIS = 1, num_dims)\n",
    "    pc_central = layers.Lambda(lambda x: tf.expand_dims(x, axis=-2))(pc_central)\n",
    "    # Points are repeated k-times along new dimension ==> (batch, num_points, k, num_dims)\n",
    "    pc_central = layers.Lambda(lambda x: tf.tile(x[0], [1, 1, x[1], 1]))( (pc_central, k) )\n",
    "\n",
    "    pc_temp2 = layers.Lambda(lambda x: tf.subtract(x[0], x[1]))( (pc_neighbors, pc_central) )\n",
    "    edge_feature = layers.Lambda(lambda x: tf.concat((x[0], x[1]), axis=-1))((pc_central, pc_temp2))\n",
    "    return edge_feature\n",
    "\n",
    "class CustomOrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, num_features=3, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.eye = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'num_features': self.num_features, 'l2reg': self.l2reg}\n",
    "\n",
    "\n",
    "def gnn_tnet(inputs, num_dims, tnet_shapes, bn=False):\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "    for filt in tnet_shapes[0]:\n",
    "        x = gnn_conv2d(inputs, filters=filt, kernel_size=[1,1], bn=bn)\n",
    "    # x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "    for filt in tnet_shapes[1]:\n",
    "        x = gnn_conv2d(inputs, filters=filt, kernel_size=[1,1], bn=bn)\n",
    "    x = layers.GlobalMaxPooling2D(keepdims=True)(x)\n",
    "    x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], y[2])))( [x, batch_size, x.shape[-1]] )\n",
    "\n",
    "    for neur in tnet_shapes[2]:\n",
    "        x = gnn_dense(x, neur, bn)\n",
    "    \n",
    "    bias = keras.initializers.Constant(np.eye(num_dims).flatten())\n",
    "    reg = CustomOrthogonalRegularizer()\n",
    "    x = layers.Dense(\n",
    "        num_dims * num_dims,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "        activity_regularizer=reg,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_dims, num_dims))(x)\n",
    "    return feat_T\n",
    "\n",
    "# function to test custom losses\n",
    "def custom_loss(pred, labels):\n",
    "    # loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=labels, logits=pred, label_smoothing=0.2)\n",
    "    # classify_loss = tf.reduce_mean(loss)\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.math.square(tf.math.subtract(labels, pred)), axis=-1))\n",
    "    return loss\n",
    "\n",
    "'''\n",
    "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "tf.math.l2_normalize\n",
    "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "'''\n",
    "\n",
    "# Callback to save good models. Threshold is on validation accuracy.\n",
    "class ValAccThresh_CB(keras.callbacks.Callback):\n",
    "    def __init__(self, thresh=0.85, experiments_path=\"experiments/\", test_name=\"test\"):\n",
    "        self.thresh = thresh\n",
    "        super(keras.callbacks.Callback, self).__init__()\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "        self.stopped_epoch = 0\n",
    "        self.experiments_path = experiments_path\n",
    "        self.test_name = test_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # self.current_epoch += 1\n",
    "        val_key = \"\"\n",
    "        for k in logs.keys():\n",
    "            if \"val\" in k and \"accuracy\" in k:\n",
    "                val_key = k\n",
    "                break\n",
    "        assert logs.get(val_key) != None, print(\" Validation Accuracy not found\", self.thresh, logs.get(val_key), val_key, logs.get(val_key) == None)\n",
    "\n",
    "        current = logs.get(val_key)\n",
    "        # current = logs.get(\"val_sparse_categorical_accuracy\")\n",
    "        # current = logs.get(\"val_accuracy\")\n",
    "        if current >= self.thresh:\n",
    "            self.thresh = current\n",
    "            self.model.save_weights(self.experiments_path+self.test_name+\"/best_weights/cp-\"+str(epoch)+\".ckpt\")\n",
    "            print(\" New good model saved.\")\n",
    "\n",
    "# Callback to save history for post-processing\n",
    "# filename=experiments_path+test_name+\"/history.csv\"\n",
    "# history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "# test_name = \"test_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(inputs, \n",
    "        num_points, num_dims, k,\n",
    "        tnet_shape,\n",
    "        conv_gnns,\n",
    "        dense_gnn,\n",
    "        classes=2):\n",
    "    '''\n",
    "        Returns the outputs of the model to be compiled.\n",
    "        ---\n",
    "        Arguments: \n",
    "        * inputs:       Expected (None, 3). instance of tf.Input.\n",
    "        * num_points:   Number of points per point cloud. Default is 200\n",
    "        * num_dims:     Number of dimensions per point. Default is 3 (x, y, z)\n",
    "        * k:            K nearest neighbors\n",
    "        * tnet_shape:   Array of three lists. (each list's length is the number of layers for that section)\n",
    "                        1st is a list of filters for convolutional layers before reduce_max.\n",
    "                        2nd is a list of filters for convolutional layers after reduce_max.\n",
    "                        3rd is a list of neurons for dense layers after max pooling.\n",
    "        * conv_gnns:    list. Each row is composed of two lists.\n",
    "                        1st is a list of filters for convolutional layers before computing edge features.\n",
    "                        2nd is a list of filters for convolutional layers after computing edge features.\n",
    "        * dense_gnn:    list of neuorns for dense layers at the end of the network.\n",
    "        * classes:      number of classes to classify.\n",
    "    '''\n",
    "\n",
    "    adj = lambda_get_adj_matr(inputs)\n",
    "    nn_idxs = lambda_knn(adj, k)\n",
    "    edge_feats = lambda_edge_feature(inputs, nn_idxs, k, num_points, num_dims)\n",
    "    feat_T = gnn_tnet(edge_feats, num_dims, tnet_shape, bn=True)\n",
    "    pc_tf = layers.Dot(axes=(-1, -2))([inputs, feat_T]) # Apply affine transformation to input features\n",
    "\n",
    "    adj = lambda_get_adj_matr(pc_tf)\n",
    "    nn_idxs = lambda_knn(adj, k)\n",
    "    edge_feats = lambda_edge_feature(pc_tf, nn_idxs, k, num_points, num_dims)\n",
    "\n",
    "    for l in conv_gnns:\n",
    "        x = edge_feats\n",
    "        for gc_filt in l[0]:\n",
    "            x = gnn_conv2d(x, gc_filt, [1,1], bn=True)\n",
    "        x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "        x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[0][-1])))( [x, tf.shape(inputs)[0]] )\n",
    "\n",
    "        adj = lambda_get_adj_matr(x)\n",
    "        nn_idxs = lambda_knn(adj, k)\n",
    "        edge_feats = lambda_edge_feature(x, nn_idxs, k, num_points, l[0][-1])\n",
    "        x = edge_feats\n",
    "        for gc_filt in l[1]:\n",
    "            x = gnn_conv2d(x, gc_filt, [1,1], bn=True)\n",
    "        x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "        x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[1][-1])))( [x, tf.shape(inputs)[0]] )\n",
    "\n",
    "    for w_ in dense_gnn:\n",
    "        x = gnn_dense(x, w_, bn=True)\n",
    "\n",
    "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    return outputs\n",
    "\n",
    "def objective(trial):\n",
    "    test_name = \"fixed_test_\"+str(trial.number)\n",
    "    filename=experiments_path+test_name+\"/history.csv\"\n",
    "    history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "    ############################ HyperParameter Setup ############################\n",
    "    ######################### Check build_model for docs #########################\n",
    "    k = trial.suggest_int('k', 5,75) #30\n",
    "    batch_size = trial.suggest_int('batch_size', 8,128) #16\n",
    "    # tnet_before_max = trial.suggest_int('tnet_before_max', 1,3)\n",
    "    # tnet_before = []\n",
    "    # for i in range(tnet_before_max):\n",
    "    #     tnet_before.append(trial.suggest_int('tnet_beforemax_layer_'+str(i), 8,128))\n",
    "    # tnet_after = []\n",
    "    # tnet_after_max = trial.suggest_int('tnet_after_max', 1,3)\n",
    "    # for i in range(tnet_after_max):\n",
    "    #     tnet_after.append(trial.suggest_int('tnet_aftermax_layer_'+str(i), 8,128))\n",
    "    # tnet_dense = []\n",
    "    # tnet_dense_layers = trial.suggest_int('tnet_dense_layers', 1,3)\n",
    "    # for i in range(tnet_dense_layers):\n",
    "    #     tnet_dense.append(trial.suggest_int('tnet_dense_layer_'+str(i), 16,256))\n",
    "    tnet_before = [32]\n",
    "    tnet_after = [128]\n",
    "    tnet_dense = [128, 128]\n",
    "    tnet_shape = [tnet_before, tnet_after, tnet_dense]\n",
    "\n",
    "    # gc_layers = trial.suggest_int('gc_layers', 1,3) #1\n",
    "    conv_gnns = []\n",
    "    # for _ in range(gc_layers):\n",
    "    #     before_edge_gcl = trial.suggest_int('before_edge_gcl', 1,3) #2\n",
    "    #     after_edge_gcl = trial.suggest_int('after_edge_gcl', 1,3) #2\n",
    "\n",
    "    #     bfr_edge = []\n",
    "    #     for i in range(before_edge_gcl):\n",
    "    #         bfr_edge.append(trial.suggest_int('before_edge_gcl_'+str(i), 8,128))\n",
    "            \n",
    "    #     aft_edge = []\n",
    "    #     for i in range(after_edge_gcl):\n",
    "    #         aft_edge.append(trial.suggest_int('after_edge_gcl_'+str(i), 8,128))\n",
    "            \n",
    "    #     conv_gnns.append([bfr_edge, aft_edge])\n",
    "    bfr_edge = [32, 64]\n",
    "    aft_edge = [128]\n",
    "    conv_gnns.append([bfr_edge, aft_edge])\n",
    "    # dense_layers = trial.suggest_int('dense_layers', 1,3)\n",
    "    # dense_gnn = []\n",
    "    # for i in range(dense_layers):\n",
    "    #     dense_gnn.append(trial.suggest_int('dense_layer_'+str(i), 16,256))\n",
    "    dense_gnn = [256, 128]\n",
    "        \n",
    "    lr = trial.suggest_float('lr', 0.0001, 0.1)\n",
    "    steps_per_epoch=trial.suggest_int('steps_per_epoch', 15,100)\n",
    "\n",
    "    validation_steps=25     # Static\n",
    "    num_points = 200        # Static\n",
    "    num_dims = 3            # Static\n",
    "    ##############################################################################\n",
    "\n",
    "    inputs = keras.Input(shape=(None, num_dims))\n",
    "    \n",
    "    outputs = build_model(inputs,\n",
    "                    num_points, num_dims, k,\n",
    "                    tnet_shape,\n",
    "                    conv_gnns,\n",
    "                    dense_gnn\n",
    "                )\n",
    "    model = keras.Model(inputs=[inputs], outputs=outputs, name=\"gnn_pointnet\")\n",
    "\n",
    "    opt_pi = tf.optimizers.Adam(learning_rate =  lr )\n",
    "    # opt_pi = tf.optimizers.RMSprop(learning_rate =  lr )\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=opt_pi, metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    # model.compile(loss=tf.nn.sparse_softmax_cross_entropy_with_logits , optimizer=opt_pi, metrics=['accuracy'])\n",
    "    # model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=opt_pi, metrics=['accuracy'])\n",
    "    # model.compile(loss=custom_loss, optimizer=opt_pi, metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_path = experiments_path+test_name+\"/cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, \n",
    "        verbose=0, \n",
    "        save_weights_only=False,\n",
    "        save_freq=10*batch_size)\n",
    "        \n",
    "    latest = tf.train.latest_checkpoint(experiments_path+test_name+\"/\")\n",
    "    if latest:\n",
    "        model.load_weights(latest)\n",
    "        latest_ep = int(latest.split('/')[-1].split('-')[-1].split('.')[0])\n",
    "        print(\" Model loaded correctly:\", latest, \" - Epoch \", latest_ep)\n",
    "    else:\n",
    "        print(\" The model could not be loaded properly: \", latest)\n",
    "        model.save(checkpoint_path.format(epoch=0))\n",
    "        latest_ep = 0\n",
    "\n",
    "    # Use CPU as default due to GPU's memory issues\n",
    "    with tf.device('/CPU:0'):\n",
    "        history = model.fit(\n",
    "            train_x, \n",
    "            train_y, \n",
    "            \n",
    "            initial_epoch=latest_ep,\n",
    "            batch_size=batch_size, \n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=validation_steps,\n",
    "\n",
    "            validation_split=0.3,\n",
    "            epochs=150 - latest_ep, # Train for 150 epochs to find the configuration that can later be trained for more epochs.\n",
    "            shuffle=True,\n",
    "            callbacks=[ValAccThresh_CB(thresh=0.9), cp_callback, history_logger],\n",
    "            use_multiprocessing=False,\n",
    "            workers=8,\n",
    "        )\n",
    "    return np.mean(history.history['val_sparse_categorical_accuracy'][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually train\n",
    "storage = optuna.storages.RDBStorage(url=\"sqlite:///gnn_fixed.db\", engine_kwargs={\"connect_args\": {\"timeout\": 5}})\n",
    "study = optuna.create_study(study_name=\"gnn_denoising_fixed\", storage=storage, load_if_exists=True, direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Configurations Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible input shapes: axis values 128 (at axis -1) != 3 (at axis -2). Full input shapes: (None, None, 32, 128), (None, 3, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb Cella 8\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m conv_gnns \u001b[39m=\u001b[39m [[[\u001b[39m128\u001b[39m], [\u001b[39m128\u001b[39m, \u001b[39m64\u001b[39m]]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m dense_gnn \u001b[39m=\u001b[39m [\u001b[39m128\u001b[39m,\u001b[39m64\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m outputs \u001b[39m=\u001b[39m build_model(inputs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m                     num_points, num_dims, k,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                     tnet_shape,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                     conv_gnns,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                     dense_gnn\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                 )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39m[inputs], outputs\u001b[39m=\u001b[39moutputs, name\u001b[39m=\u001b[39mtest_name\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnet\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m opt_pi \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mlr)\n",
      "\u001b[1;32m/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb Cella 8\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(inputs, num_points, num_dims, k, tnet_shape, conv_gnns, dense_gnn, classes)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# x = tf.reduce_max(x, axis=-2, keepdims=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[0][-1])))( [x, tf.shape(inputs)[0]] )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# edge_feats = lambda_edge_feature(x, nn_idxs, k, num_points, l[0][-1])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# x = edge_feats\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m feat_T \u001b[39m=\u001b[39m gnn_tnet(x, num_dims, tnet_shape[\u001b[39m1\u001b[39m], bn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mDot(axes\u001b[39m=\u001b[39;49m(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))([x, feat_T]) \u001b[39m# Apply affine transformation to input features\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m gc_filt \u001b[39min\u001b[39;00m l[\u001b[39m1\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X10sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     x \u001b[39m=\u001b[39m gnn_conv2d(x, gc_filt, [\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m], bn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/layers/merge.py:690\u001b[0m, in \u001b[0;36mDot.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    688\u001b[0m   axes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\n\u001b[1;32m    689\u001b[0m \u001b[39mif\u001b[39;00m shape1[axes[\u001b[39m0\u001b[39m]] \u001b[39m!=\u001b[39m shape2[axes[\u001b[39m1\u001b[39m]]:\n\u001b[0;32m--> 690\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    691\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mIncompatible input shapes: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    692\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39maxis values \u001b[39m\u001b[39m{\u001b[39;00mshape1[axes[\u001b[39m0\u001b[39m]]\u001b[39m}\u001b[39;00m\u001b[39m (at axis \u001b[39m\u001b[39m{\u001b[39;00maxes[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m) != \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    693\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mshape2[axes[\u001b[39m1\u001b[39m]]\u001b[39m}\u001b[39;00m\u001b[39m (at axis \u001b[39m\u001b[39m{\u001b[39;00maxes[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m). \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    694\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFull input shapes: \u001b[39m\u001b[39m{\u001b[39;00mshape1\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mshape2\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible input shapes: axis values 128 (at axis -1) != 3 (at axis -2). Full input shapes: (None, None, 32, 128), (None, 3, 3)"
     ]
    }
   ],
   "source": [
    "test_name = \"manual_test_reg_06\"\n",
    "\n",
    "k=32\n",
    "batch_size=16\n",
    "steps_per_epoch=16\n",
    "validation_steps=25\n",
    "lr = 0.001\n",
    "\n",
    "num_points=200\n",
    "num_dims=3\n",
    "\n",
    "checkpoint_path = experiments_path+test_name+\"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=0,\n",
    "    save_weights_only=False,\n",
    "    save_freq=10*steps_per_epoch)\n",
    "\n",
    "inputs = keras.Input(shape=(None, 3))\n",
    "\n",
    "tnet_shape = [[64, 64], [64], [256,512]]\n",
    "conv_gnns = [[[128, 64], [64, 64]]]\n",
    "dense_gnn = [128,64]\n",
    "\n",
    "outputs = build_model(inputs,\n",
    "                    num_points, num_dims, k,\n",
    "                    tnet_shape,\n",
    "                    conv_gnns,\n",
    "                    dense_gnn\n",
    "                )\n",
    "\n",
    "model = keras.Model(inputs=[inputs], outputs=outputs, name=test_name+\"net\")\n",
    "\n",
    "\n",
    "opt_pi = tf.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=opt_pi, metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# Try to load the model. If it does not exist, create it.\n",
    "# latest = tf.train.latest_checkpoint(experiments_path+test_name+\"/\")\n",
    "latest = sorted([ f.path for f in os.scandir(experiments_path+test_name) if f.is_dir() ])[-1] \\\n",
    "    if os.path.isdir(experiments_path+test_name) else None\n",
    "\n",
    "if latest:\n",
    "    # https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "    # model.load(latest)\n",
    "    model = tf.keras.models.load_model(latest)\n",
    "    latest_ep = int(latest.split('/')[-1].split('-')[-1].split('.')[0])\n",
    "    print(\" Model loaded correctly:\", latest, \" - Epoch \", latest_ep)\n",
    "else:\n",
    "    print(\" The model at \", experiments_path+test_name+\"/\", \"could not be loaded properly: \", latest)\n",
    "    model.save(checkpoint_path.format(epoch=0))\n",
    "    latest_ep = 0\n",
    "\n",
    "# This grants no overwriting of the history file\n",
    "filename=experiments_path+test_name+\"/history\"+str(latest_ep)+\".csv\"\n",
    "history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "\n",
    "# Use CPU as default due to GPU's memory issues\n",
    "with tf.device('/CPU:0'):\n",
    "    history = model.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        \n",
    "        initial_epoch=latest_ep,\n",
    "        batch_size=batch_size, \n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "\n",
    "        validation_split=0.3,\n",
    "        epochs=5000,\n",
    "        shuffle=True,\n",
    "        callbacks=[ValAccThresh_CB(thresh=0.85, experiments_path=experiments_path, test_name=test_name), cp_callback, history_logger],\n",
    "        use_multiprocessing=False,\n",
    "        workers=8,\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
