{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph NN for MMWave Sensor filtering\n",
    "The idea is to train a classifier to distinguish between fake points and actual ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 09:19:19.986511: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-06-29 09:19:19.986549: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data shape:  (7263268, 7)\n",
      "Points per cloud:  200\n",
      "Skipping because:  (68, 7) 7263200 7263400\n",
      "Skipped 1 point clouds\n",
      " Data shape:  (1148223, 7)\n",
      "Points per cloud:  200\n",
      "Skipping because:  (23, 7) 1148200 1148400\n",
      "Skipped 1 point clouds\n",
      " Data shape:  (3411821, 7)\n",
      "Points per cloud:  200\n",
      "Skipping because:  (21, 7) 3411800 3412000\n",
      "Skipped 1 point clouds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Requirements:\n",
    "     * TF:      2.7.0\n",
    "     * Keras:   2.7.0\n",
    "     * numpy:   1.22.3\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "'''\n",
    "    Setting Up matplotlib for paper compliant figures\n",
    "    (this should avoid problems when compiling latex stuff)\n",
    "'''\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Where datasets are stored\n",
    "run_path = \"data/\"\n",
    "experiments_path = \"experiments/\"\n",
    "\n",
    "from utils import load_train_test_data, get_data_and_label2\n",
    "\n",
    "valid_filter = [9, 69, 73, 3, 55]\n",
    "train_filter=[4, 5, 7, 8, 49, 51, 54, 58, 59, 62, 63, 65, 66, 68, 71, 72, 75, 76, 77, 79, 81, 82, 84, 86, 87, 89, 90, 93, 95, 97, 98]\n",
    "test_filter=[53, 67, 74, 80, 88, 96, 61, 6, 64, 92, 70, 85, 50, 56, 57]\n",
    "\n",
    "\n",
    "# Original separation\n",
    "# train_filter=[4,6,8,9,50,51,53,54,56,58,59,61,62,63,65,66,67,69]\n",
    "# test_filter=[3,7,49,55,57,64,68,70]\n",
    "\n",
    "# dataset = load_datasets(run_path)\n",
    "train, valid, test = load_train_test_data(run_path, train_filter=train_filter, valid_filter=valid_filter, test_filter=test_filter)\n",
    "\n",
    "# Separate Train and Test data\n",
    "# d_len = int(len(dataset)*0.7)\n",
    "# train, test = dataset[:d_len], dataset[d_len:]\n",
    "\n",
    "\n",
    "# Shuffle point cloud dataset\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(valid)\n",
    "np.random.shuffle(test)\n",
    "\n",
    "# Get X and Y data for training\n",
    "train_x, train_y = get_data_and_label2(train)\n",
    "valid_x, valid_y = get_data_and_label2(valid)\n",
    "test_x, test_y = get_data_and_label2(test)\n",
    "\n",
    "# len(train_x), len(valid_x), len(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Graph CNN \n",
    "#### https://github.com/WangYueFt/dgcnn\n",
    "#### https://stackoverflow.com/questions/37009647/compute-pairwise-distance-in-a-batch-without-replicating-tensor-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gnn_conv2d(inputs,\n",
    "#             filters,\n",
    "#             kernel_size,\n",
    "#             stride=[1],\n",
    "#             padding='SAME',\n",
    "#             use_xavier=True,\n",
    "#             stddev=1e-3,\n",
    "#             activation_fn=tf.nn.relu,\n",
    "#             bn=False):\n",
    "\n",
    "#     x = layers.Conv1D(\n",
    "#         filters, \n",
    "#         kernel_size, \n",
    "#         strides=stride, \n",
    "#         padding=padding,\n",
    "#         activation=activation_fn,\n",
    "#         kernel_initializer='glorot_uniform' if use_xavier else keras.initializers.TruncatedNormal(stddev=stddev),\n",
    "#         bias_initializer='zeros'\n",
    "#     )(inputs)\n",
    "\n",
    "#     if bn: x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(0.4)(x)\n",
    "#     return x\n",
    "\n",
    "# def gnn_dense(inputs,\n",
    "#             units,\n",
    "#             use_xavier=True,\n",
    "#             stddev=1e-3,\n",
    "#             activation_fn=tf.nn.relu,\n",
    "#             bn=False):\n",
    "            \n",
    "#     x = layers.Dense(units,\n",
    "#         activation=activation_fn,\n",
    "#         kernel_initializer='glorot_uniform' if use_xavier else keras.initializers.TruncatedNormal(stddev=stddev),\n",
    "#         bias_initializer='zeros'\n",
    "#     )(inputs)\n",
    "\n",
    "#     if bn: x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(0.4)(x)\n",
    "#     return x\n",
    "\n",
    "# def lambda_get_adj_matr(input):\n",
    "#     pcT = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(input)\n",
    "#     pc_inn = layers.Lambda(lambda x: tf.matmul(x[0], x[1]))( (input, pcT) )\n",
    "#     pc2 = layers.Lambda(lambda x: tf.reduce_sum(tf.square(x), axis=-1, keepdims=True))(input)\n",
    "#     pc2T = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(pc2)\n",
    "#     output = layers.Lambda(lambda x: x[0] + -2 * x[1] + x[2])( (pc2, pc_inn, pc2T) )\n",
    "#     # Uncomment line below to use reciprocal of adj matrix (1/distance)\n",
    "#     # output = layers.Lambda(lambda x: tf.math.reciprocal(x))(output)\n",
    "#     return output\n",
    "\n",
    "# def lambda_knn(adj, k=20):\n",
    "#     x = layers.Lambda(lambda x: tf.math.top_k(-x[0], x[1]))( (adj, k) )\n",
    "#     return x.indices\n",
    "\n",
    "# def lambda_edge_feature(inputs, nn_idxs, k=20, num_points=200, num_dims=3):\n",
    "\n",
    "#     pc_central = inputs\n",
    "#     batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "#     idx_ = layers.Lambda(lambda x: tf.range(x[0]) * x[1])( (batch_size, num_points) )\n",
    "#     idx_ = layers.Lambda(lambda x: tf.reshape(x[0], (x[1], 1, 1)))( (idx_, batch_size) )\n",
    "#     # Adding to list of idxs of k points the points themselves\n",
    "#     pc_temp1 = layers.Lambda(lambda x: x[0]+x[1])( (nn_idxs, idx_) )\n",
    "\n",
    "#     # Flattening of points into a list of coordinates (x,y,z)\n",
    "#     pc_flat = layers.Lambda(lambda x: tf.reshape(x[0], [-1, x[1]]))( (inputs, num_dims) )\n",
    "\n",
    "#     # Collect points from computed idxs\n",
    "#     pc_neighbors = layers.Lambda(lambda x: tf.gather(x[0], x[1]) )( (pc_flat, pc_temp1) )\n",
    "\n",
    "#     # Reshape points into shape (batch, num_points, NEW_AXIS = 1, num_dims)\n",
    "#     pc_central = layers.Lambda(lambda x: tf.expand_dims(x, axis=-2))(pc_central)\n",
    "#     # Points are repeated k-times along new dimension ==> (batch, num_points, k, num_dims)\n",
    "#     pc_central = layers.Lambda(lambda x: tf.tile(x[0], [1, 1, x[1], 1]))( (pc_central, k) )\n",
    "\n",
    "#     pc_temp2 = layers.Lambda(lambda x: tf.subtract(x[0], x[1]))( (pc_neighbors, pc_central) )\n",
    "#     edge_feature = layers.Lambda(lambda x: tf.concat((x[0], x[1]), axis=-1))((pc_central, pc_temp2))\n",
    "#     return edge_feature\n",
    "\n",
    "# class CustomOrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "#     def __init__(self, num_features=3, l2reg=0.0005):\n",
    "#         self.num_features = num_features\n",
    "#         self.l2reg = l2reg\n",
    "#         self.eye = tf.eye(num_features)\n",
    "\n",
    "#     def __call__(self, x):\n",
    "#         x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "#         xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "#         xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "#         return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         return {'num_features': self.num_features, 'l2reg': self.l2reg}\n",
    "\n",
    "# def gnn_tnet(inputs, num_dims, tnet_shapes, bn=False):\n",
    "#     batch_size = tf.shape(inputs)[0]\n",
    "#     for filt in tnet_shapes[0]:\n",
    "#         x = gnn_conv2d(inputs, filters=filt, kernel_size=[1], bn=bn)\n",
    "#     # x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "#     # for filt in tnet_shapes[1]:\n",
    "#     #     x = gnn_conv2d(inputs, filters=filt, kernel_size=[1], bn=bn)\n",
    "#     x = layers.GlobalMaxPooling1D(keepdims=True)(x)\n",
    "#     # print(\" [tnet] global maxpool 2d shape: \", x.shape)\n",
    "#     x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], y[2])))( [x, batch_size, x.shape[-1]] )\n",
    "#     # print(\" [tnet] reshape: \", x.shape)\n",
    "\n",
    "#     for neur in tnet_shapes[2]:\n",
    "#         x = gnn_dense(x, neur, bn)\n",
    "    \n",
    "#     bias = keras.initializers.Constant(np.eye(num_dims).flatten())\n",
    "#     reg = CustomOrthogonalRegularizer(num_features=num_dims)\n",
    "#     x = layers.Dense(\n",
    "#         num_dims * num_dims,\n",
    "#         kernel_initializer=\"zeros\",\n",
    "#         bias_initializer=bias,\n",
    "#         activity_regularizer=reg,\n",
    "#     )(x)\n",
    "#     feat_T = layers.Reshape((num_dims, num_dims), name='tnet_last_reshape'+str(np.round(time.time(), 5)))(x)\n",
    "#     return feat_T\n",
    "\n",
    "# # function to test custom losses\n",
    "# def custom_loss(pred, labels):\n",
    "#     # loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=labels, logits=pred, label_smoothing=0.2)\n",
    "#     # classify_loss = tf.reduce_mean(loss)\n",
    "#     loss = tf.reduce_mean(tf.reduce_sum(tf.math.square(tf.math.subtract(labels, pred)), axis=-1))\n",
    "#     return loss\n",
    "\n",
    "# '''\n",
    "# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "# tf.math.l2_normalize\n",
    "# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "# '''\n",
    "\n",
    "# # Callback to save good models. Threshold is on validation accuracy.\n",
    "# class ValAccThresh_CB(keras.callbacks.Callback):\n",
    "#     def __init__(self, thresh=0.85, experiments_path=\"experiments/\", test_name=\"test\"):\n",
    "#         self.thresh = thresh\n",
    "#         super(keras.callbacks.Callback, self).__init__()\n",
    "#         # best_weights to store the weights at which the minimum loss occurs.\n",
    "#         self.best_weights = None\n",
    "#         self.stopped_epoch = 0\n",
    "#         self.experiments_path = experiments_path\n",
    "#         self.test_name = test_name\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         # self.current_epoch += 1\n",
    "#         val_key = \"\"\n",
    "#         for k in logs.keys():\n",
    "#             if \"val\" in k and \"accuracy\" in k:\n",
    "#                 val_key = k\n",
    "#                 break\n",
    "#         assert logs.get(val_key) != None, print(\" Validation Accuracy not found\", self.thresh, logs.get(val_key), val_key, logs.get(val_key) == None)\n",
    "\n",
    "#         current = logs.get(val_key)\n",
    "#         # current = logs.get(\"val_sparse_categorical_accuracy\")\n",
    "#         # current = logs.get(\"val_accuracy\")\n",
    "#         if current >= self.thresh:\n",
    "#             self.thresh = current\n",
    "#             self.model.save_weights(self.experiments_path+self.test_name+\"/best_weights/cp-\"+str(epoch)+\".ckpt\")\n",
    "#             print(\" New good model saved.\")\n",
    "\n",
    "# Callback to save history for post-processing\n",
    "# filename=experiments_path+test_name+\"/history.csv\"\n",
    "# history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "# test_name = \"test_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gnn_conv2d, gnn_dense, \\\n",
    "    lambda_get_adj_matr, lambda_knn, lambda_edge_feature, \\\n",
    "    CustomOrthogonalRegularizer, gnn_tnet, custom_loss, ValAccThresh_CB\n",
    "\n",
    "def build_model(inputs, \n",
    "        num_points, num_dims, k,\n",
    "        tnet_shape,\n",
    "        conv_gnns,\n",
    "        dense_gnn,\n",
    "        classes=1):\n",
    "    '''\n",
    "        Returns the outputs of the model to be compiled.\n",
    "        ---\n",
    "        Arguments: \n",
    "        * inputs:       Expected (None, 3). instance of tf.Input.\n",
    "        * num_points:   Number of points per point cloud. Default is 200\n",
    "        * num_dims:     Number of dimensions per point. Default is 3 (x, y, z)\n",
    "        * k:            K nearest neighbors\n",
    "        * tnet_shape:   Array of three lists. (each list's length is the number of layers for that section)\n",
    "                        1st is a list of filters for convolutional layers before reduce_max.\n",
    "                        2nd is a list of filters for convolutional layers after reduce_max.\n",
    "                        3rd is a list of neurons for dense layers after max pooling.\n",
    "        * conv_gnns:    list. Each row is composed of two lists.\n",
    "                        1st is a list of filters for convolutional layers before computing edge features.\n",
    "                        2nd is a list of filters for convolutional layers after computing edge features.\n",
    "        * dense_gnn:    list of neuorns for dense layers at the end of the network.\n",
    "        * classes:      number of classes to classify.\n",
    "    '''\n",
    "\n",
    "    # adj = lambda_get_adj_matr(inputs)\n",
    "    # nn_idxs = lambda_knn(adj, k)\n",
    "    # edge_feats = lambda_edge_feature(inputs, nn_idxs, k, num_points, num_dims)\n",
    "    x = inputs\n",
    "    # x = gnn_dense(inputs, 8, bn=False)\n",
    "    feat_T = gnn_tnet(x, num_dims, tnet_shape[0], bn=False)\n",
    "    # print(\" [main] tnet shape: \", inputs.shape, feat_T.shape)\n",
    "    pc_tf = layers.Dot(axes=(-1, -2))([x, feat_T]) # Apply affine transformation to input features\n",
    "    # print(\" [main] dot shape: \", pc_tf.shape)\n",
    "\n",
    "    # adj = lambda_get_adj_matr(pc_tf)\n",
    "    # nn_idxs = lambda_knn(adj, k)\n",
    "    # edge_feats = lambda_edge_feature(pc_tf, nn_idxs, k, num_points, num_dims)\n",
    "    # x = edge_feats\n",
    "    x = pc_tf\n",
    "\n",
    "    for l in conv_gnns:\n",
    "\n",
    "        for gc_filt in l[0]:\n",
    "            x = gnn_conv2d(x, gc_filt, [1], bn=False)\n",
    "        \n",
    "        feat_T = gnn_tnet(x, l[0][-1], tnet_shape[1], bn=False)\n",
    "        # print(\" [main] tnet shape: \", inputs.shape, feat_T.shape)\n",
    "        x = layers.Dot(axes=(-1, -2))([x, feat_T]) # Apply affine transformation to input features\n",
    "        # x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "        # x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[0][-1])))( [x, tf.shape(inputs)[0]] )\n",
    "\n",
    "        # adj = lambda_get_adj_matr(x)\n",
    "        # nn_idxs = lambda_knn(adj, k)\n",
    "        # edge_feats = lambda_edge_feature(x, nn_idxs, k, num_points, l[0][-1])\n",
    "        # x = edge_feats\n",
    "\n",
    "        # feat_T = gnn_tnet(x, x.shape[-1], tnet_shape[1], bn=True)\n",
    "        # print(\" [main] tnet SHAPES: \", x.shape, feat_T.shape)\n",
    "        # x = layers.Dot(axes=(-1, -2))([x, feat_T])\n",
    "        # print(\" [main] DOT SHAPE: \", x.shape)\n",
    "\n",
    "        # adj = lambda_get_adj_matr(x)\n",
    "        # nn_idxs = lambda_knn(adj, k)\n",
    "        # edge_feats = lambda_edge_feature(x, nn_idxs, k, num_points, x.shape[-1])\n",
    "\n",
    "        for gc_filt in l[1]:\n",
    "            x = gnn_conv2d(x, gc_filt, [1], bn=False)\n",
    "        # x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "        # x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[1][-1])))( [x, tf.shape(inputs)[0]] )\n",
    "\n",
    "    # print(tf.shape(x))\n",
    "    # x = layers.GlobalMaxPooling2D(keepdims=True)(x)\n",
    "    # x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], y[2])))( [x, x.shape[0], x.shape[-1]] )\n",
    "    # print(tf.shape(x))\n",
    "    # x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], y[2])))( [x, tf.shape(x)[0], x.shape[-1]] )\n",
    "    # x = layers.GlobalMaxPooling1D()(x)\n",
    "    # print(\" [main] shape: \", x.shape)\n",
    "    for w_ in dense_gnn:\n",
    "        x = gnn_dense(x, w_, bn=False)\n",
    "        # x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    outputs = layers.Dense(classes, activation=\"relu\")(x)\n",
    "    # print(tf.shape(outputs))\n",
    "    return outputs\n",
    "\n",
    "def objective(trial):\n",
    "    test_name = \"fixed_test_\"+str(trial.number)\n",
    "    filename=experiments_path+test_name+\"/history.csv\"\n",
    "    history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "    ############################ HyperParameter Setup ############################\n",
    "    ######################### Check build_model for docs #########################\n",
    "    k = trial.suggest_int('k', 5,75) #30\n",
    "    batch_size = trial.suggest_int('batch_size', 8,128) #16\n",
    "    # tnet_before_max = trial.suggest_int('tnet_before_max', 1,3)\n",
    "    # tnet_before = []\n",
    "    # for i in range(tnet_before_max):\n",
    "    #     tnet_before.append(trial.suggest_int('tnet_beforemax_layer_'+str(i), 8,128))\n",
    "    # tnet_after = []\n",
    "    # tnet_after_max = trial.suggest_int('tnet_after_max', 1,3)\n",
    "    # for i in range(tnet_after_max):\n",
    "    #     tnet_after.append(trial.suggest_int('tnet_aftermax_layer_'+str(i), 8,128))\n",
    "    # tnet_dense = []\n",
    "    # tnet_dense_layers = trial.suggest_int('tnet_dense_layers', 1,3)\n",
    "    # for i in range(tnet_dense_layers):\n",
    "    #     tnet_dense.append(trial.suggest_int('tnet_dense_layer_'+str(i), 16,256))\n",
    "    tnet_before = [32]\n",
    "    tnet_after = [128]\n",
    "    tnet_dense = [128, 128]\n",
    "    tnet_shape = [tnet_before, tnet_after, tnet_dense]\n",
    "\n",
    "    # gc_layers = trial.suggest_int('gc_layers', 1,3) #1\n",
    "    conv_gnns = []\n",
    "    # for _ in range(gc_layers):\n",
    "    #     before_edge_gcl = trial.suggest_int('before_edge_gcl', 1,3) #2\n",
    "    #     after_edge_gcl = trial.suggest_int('after_edge_gcl', 1,3) #2\n",
    "\n",
    "    #     bfr_edge = []\n",
    "    #     for i in range(before_edge_gcl):\n",
    "    #         bfr_edge.append(trial.suggest_int('before_edge_gcl_'+str(i), 8,128))\n",
    "            \n",
    "    #     aft_edge = []\n",
    "    #     for i in range(after_edge_gcl):\n",
    "    #         aft_edge.append(trial.suggest_int('after_edge_gcl_'+str(i), 8,128))\n",
    "            \n",
    "    #     conv_gnns.append([bfr_edge, aft_edge])\n",
    "    bfr_edge = [32, 64]\n",
    "    aft_edge = [128]\n",
    "    conv_gnns.append([bfr_edge, aft_edge])\n",
    "    # dense_layers = trial.suggest_int('dense_layers', 1,3)\n",
    "    # dense_gnn = []\n",
    "    # for i in range(dense_layers):\n",
    "    #     dense_gnn.append(trial.suggest_int('dense_layer_'+str(i), 16,256))\n",
    "    dense_gnn = [256, 128]\n",
    "        \n",
    "    lr = trial.suggest_float('lr', 0.0001, 0.1)\n",
    "    steps_per_epoch=trial.suggest_int('steps_per_epoch', 15,100)\n",
    "\n",
    "    validation_steps=25     # Static\n",
    "    num_points = 200        # Static\n",
    "    num_dims = 3            # Static\n",
    "    ##############################################################################\n",
    "\n",
    "    inputs = keras.Input(shape=(None, num_dims))\n",
    "    \n",
    "    outputs = build_model(inputs,\n",
    "                    num_points, num_dims, k,\n",
    "                    tnet_shape,\n",
    "                    conv_gnns,\n",
    "                    dense_gnn\n",
    "                )\n",
    "    model = keras.Model(inputs=[inputs], outputs=outputs, name=\"gnn_pointnet\")\n",
    "\n",
    "    opt_pi = tf.optimizers.Adam(learning_rate =  lr )\n",
    "    # opt_pi = tf.optimizers.RMSprop(learning_rate =  lr )\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=opt_pi, metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    # model.compile(loss=tf.nn.sparse_softmax_cross_entropy_with_logits , optimizer=opt_pi, metrics=['accuracy'])\n",
    "    # model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=opt_pi, metrics=['accuracy'])\n",
    "    # model.compile(loss=custom_loss, optimizer=opt_pi, metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_path = experiments_path+test_name+\"/cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, \n",
    "        verbose=0, \n",
    "        save_weights_only=False,\n",
    "        save_freq=10*batch_size)\n",
    "        \n",
    "    latest = tf.train.latest_checkpoint(experiments_path+test_name+\"/\")\n",
    "    if latest:\n",
    "        model.load_weights(latest)\n",
    "        latest_ep = int(latest.split('/')[-1].split('-')[-1].split('.')[0])\n",
    "        print(\" Model loaded correctly:\", latest, \" - Epoch \", latest_ep)\n",
    "    else:\n",
    "        print(\" The model could not be loaded properly: \", latest)\n",
    "        model.save(checkpoint_path.format(epoch=0))\n",
    "        latest_ep = 0\n",
    "\n",
    "    # Use CPU as default due to GPU's memory issues\n",
    "    with tf.device('/CPU:0'):\n",
    "        history = model.fit(\n",
    "            train_x, \n",
    "            train_y, \n",
    "            \n",
    "            initial_epoch=latest_ep,\n",
    "            batch_size=batch_size, \n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=validation_steps,\n",
    "\n",
    "            validation_split=0.3,\n",
    "            epochs=150 - latest_ep, # Train for 150 epochs to find the configuration that can later be trained for more epochs.\n",
    "            shuffle=True,\n",
    "            callbacks=[ValAccThresh_CB(thresh=0.9), cp_callback, history_logger],\n",
    "            use_multiprocessing=False,\n",
    "            workers=8,\n",
    "        )\n",
    "    return np.mean(history.history['val_sparse_categorical_accuracy'][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually train\n",
    "storage = optuna.storages.RDBStorage(url=\"sqlite:///gnn_fixed.db\", engine_kwargs={\"connect_args\": {\"timeout\": 5}})\n",
    "study = optuna.create_study(study_name=\"gnn_denoising_fixed\", storage=storage, load_if_exists=True, direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Configurations Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 09:20:44.959511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 09:20:44.960565: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-06-29 09:20:44.960965: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-06-29 09:20:44.961315: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-06-29 09:20:44.980152: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-06-29 09:20:44.980448: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-06-29 09:20:44.980668: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/walter/catkin_ws/devel/lib:/opt/ros/noetic/lib:/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-06-29 09:20:44.980696: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-06-29 09:20:44.981261: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-29 09:20:45.290770: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 134217728 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The model at  experiments/manual_test_regress_relu_01/ could not be loaded properly:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 09:20:47.482192: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/manual_test_regress_relu_01/cp-0000.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 09:20:50.716123: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 87158400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 09:20:51.661149: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 134217728 exceeds 10% of free system memory.\n",
      "2023-06-29 09:20:51.707748: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 134217728 exceeds 10% of free system memory.\n",
      "2023-06-29 09:20:53.591995: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 52428800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mSi è verificato un arresto anomalo del kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. Esaminare il codice nelle celle per identificare una possibile causa dell'errore. Per altre informazioni, fare clic su <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a>. Per altri dettagli, vedere Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "class CustomMetric(tf.keras.metrics.Accuracy):\n",
    "\n",
    "  def __init__(self, name='custom_metric', thresh=0.1, **kwargs):\n",
    "    super(CustomMetric, self).__init__(name=name, **kwargs)\n",
    "    self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "    # self.accuracy_comp = tf.keras.metrics.sparse_categorical_crossentropy()\n",
    "    self.thresh = thresh\n",
    "    self.acc = 0.\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    # print(y_true.shape, y_pred.shape)\n",
    "    # y_true = tf.cast((y_true <= self.thresh), tf.bool)\n",
    "    # y_pred = tf.cast((tf.squeeze(y_pred) <= self.thresh), tf.bool)\n",
    "    # y_true = tf.squeeze(tf.cast(y_true == 0, tf.int32))\n",
    "    # y_pred = tf.squeeze(tf.cast(y_pred == 0, tf.int32))\n",
    "    y_true = tf.cast(y_true <= self.thresh, tf.int32)\n",
    "    y_pred = tf.cast(y_pred <= self.thresh, tf.int32)\n",
    "\n",
    "    # print(y_true.shape, y_pred.shape)\n",
    "    super().update_state(y_true, y_pred, sample_weight)\n",
    "    # self.acc = tf.keras.metrics.categorical_accuracy(y_true, y_pred)\n",
    "    # tf.print(\"[CustomMetric/UpdateState]: Acc: \", self.acc)\n",
    "    # values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
    "    # values = tf.cast(values, self.dtype)\n",
    "    # if sample_weight is not None:\n",
    "    #   sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "    #   sample_weight = tf.broadcast_to(sample_weight, values.shape)\n",
    "    #   values = tf.multiply(values, sample_weight)\n",
    "    # self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "#   def result(self):\n",
    "#     # return self.true_positives\n",
    "#     return self.acc\n",
    "  \n",
    "from utils import ValAccThresh_CB\n",
    "\n",
    "test_name = \"manual_test_regress_relu_01\"\n",
    "\n",
    "k=16\n",
    "batch_size=128\n",
    "steps_per_epoch=32\n",
    "validation_steps=25\n",
    "lr = 0.0001\n",
    "\n",
    "num_points=200\n",
    "num_dims=3\n",
    "\n",
    "checkpoint_path = experiments_path+test_name+\"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=0,\n",
    "    save_weights_only=False,\n",
    "    save_freq=10*steps_per_epoch)\n",
    "\n",
    "inputs = keras.Input(shape=(None, 3))\n",
    "\n",
    "tnet_shape = [[[32, 64, 128], [8], [128]], [[256, 512], [8], [512]]]\n",
    "conv_gnns = [[[128, 256], [256, 128]]]\n",
    "dense_gnn = [512,128]\n",
    "\n",
    "outputs = build_model(inputs,\n",
    "                    num_points, num_dims, k,\n",
    "                    tnet_shape,\n",
    "                    conv_gnns,\n",
    "                    dense_gnn\n",
    "                )\n",
    "\n",
    "model = keras.Model(inputs=[inputs], outputs=outputs, name=test_name+\"net\")\n",
    "\n",
    "\n",
    "opt_pi = tf.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.math.square(y_true*100 - y_pred*100), axis=-1)\n",
    "# model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=opt_pi, metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.compile(loss=custom_loss, optimizer=opt_pi, metrics=[CustomMetric()])\n",
    "\n",
    "# Try to load the model. If it does not exist, create it.\n",
    "# latest = tf.train.latest_checkpoint(experiments_path+test_name+\"/\")\n",
    "latest = sorted([ f.path for f in os.scandir(experiments_path+test_name) if f.is_dir() ])[-1] \\\n",
    "    if os.path.isdir(experiments_path+test_name) else None\n",
    "\n",
    "if latest:\n",
    "    # https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "    # model.load(latest)\n",
    "    model = tf.keras.models.load_model(latest, custom_objects={'CustomOrthogonalRegularizer': CustomOrthogonalRegularizer, 'custom_loss': custom_loss, 'CustomMetric': CustomMetric})\n",
    "    latest_ep = int(latest.split('/')[-1].split('-')[-1].split('.')[0])\n",
    "    print(\" Model loaded correctly:\", latest, \" - Epoch \", latest_ep)\n",
    "else:\n",
    "    print(\" The model at \", experiments_path+test_name+\"/\", \"could not be loaded properly: \", latest)\n",
    "    model.save(checkpoint_path.format(epoch=0))\n",
    "    latest_ep = 0\n",
    "\n",
    "# This grants no overwriting of the history file\n",
    "filename=experiments_path+test_name+\"/history\"+str(latest_ep)+\".csv\"\n",
    "history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "\n",
    "# Use CPU as default due to GPU's memory issues\n",
    "with tf.device('/CPU:0'):\n",
    "    history = model.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        \n",
    "        initial_epoch=latest_ep,\n",
    "        batch_size=batch_size, \n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "\n",
    "        # validation_split=0.3,\n",
    "        validation_data=(valid_x, valid_y),\n",
    "        epochs=3000,\n",
    "        shuffle=True,\n",
    "        callbacks=[ValAccThresh_CB(thresh=0.85, experiments_path=experiments_path, test_name=test_name), cp_callback, history_logger],\n",
    "        use_multiprocessing=True,\n",
    "        workers=8,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019624097"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.mean(np.square(model.predict(test_x[0:10]) - test_y[0:10, None, :]), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.losses.SparseCategoricalCrossentropy()(np.int32(test_y == 0), np.squeeze(np.int32(model.predict(test_x) == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "(10, 2) (10, 200)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Can not squeeze dim[1], expected a dimension of 1, got 200 [Op:Squeeze]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb Cella 12\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msqueeze(tf\u001b[39m.\u001b[39mcast(test_y[:\u001b[39m10\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m, tf\u001b[39m.\u001b[39mint32))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(y_pred\u001b[39m.\u001b[39mshape, y_true\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/walter/graphrnn-mmw-denoising/gnn_tf.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmetrics\u001b[39m.\u001b[39;49msparse_categorical_accuracy(y_true, y_pred,)\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/metrics.py:3601\u001b[0m, in \u001b[0;36msparse_categorical_accuracy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m   3598\u001b[0m \u001b[39m# If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\u001b[39;00m\n\u001b[1;32m   3599\u001b[0m \u001b[39mif\u001b[39;00m (y_true_rank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (y_pred_rank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mlen\u001b[39m(\n\u001b[1;32m   3600\u001b[0m     backend\u001b[39m.\u001b[39mint_shape(y_true)) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(backend\u001b[39m.\u001b[39mint_shape(y_pred))):\n\u001b[0;32m-> 3601\u001b[0m   y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49msqueeze(y_true, [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m   3602\u001b[0m y_pred \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39margmax(y_pred, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   3604\u001b[0m \u001b[39m# If the predicted output and actual output types don't match, force cast them\u001b[39;00m\n\u001b[1;32m   3605\u001b[0m \u001b[39m# to match.\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Can not squeeze dim[1], expected a dimension of 1, got 200 [Op:Squeeze]"
     ]
    }
   ],
   "source": [
    "# np.squeeze(np.int32(model.predict(test_x[:10]) == 0), axis=-1).shape\n",
    "# model.predict(test_x).shape\n",
    "# one hot encode model output\n",
    "y_pred = model.predict(test_x[:10])\n",
    "print(y_pred.shape)\n",
    "y_pred = tf.one_hot(tf.squeeze(tf.cast(y_pred <= 0.1, tf.int32)), 2)\n",
    "y_true = tf.squeeze(tf.cast(test_y[:10] <= 0.1, tf.int32))\n",
    "print(y_pred.shape, y_true.shape)\n",
    "tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred,).numpy()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
