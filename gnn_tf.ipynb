{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph NN for MMWave Sensor filtering\n",
    "The idea is to train a classifier to distinguish between fake points and actual ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 10:00:55.091098: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib::/home/walter/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2022-11-09 10:00:55.091168: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Where datasets are stored\n",
    "run_path = \"data/\"\n",
    "experiments_path = \"experiments/\"\n",
    "\n",
    "def load_datasets(init_path):\n",
    "    '''\n",
    "        Return dataset as list of pointclouds\n",
    "        ---\n",
    "        Parameters:\n",
    "        * init_path: string; path to folder holding folders of datasets\n",
    "                     (expected folder structure: 'init_path/run_X/labelled_mmw_run_X.json')\n",
    "    '''\n",
    "    data = []\n",
    "    for run in sorted(os.listdir(init_path)):\n",
    "        if \"run_\" in run:\n",
    "            data.extend(json.load(open(init_path+run+\"/labelled_mmw_\"+run+\".json\"))['labelled_mmw'])\n",
    "    return data\n",
    "\n",
    "def get_data_and_label(data, points_per_cloud=200):\n",
    "    '''\n",
    "        Return samples for training as np.array, divided as unlabelled data and related labels.\n",
    "        ---\n",
    "        Parameters:\n",
    "        * data: list of point clouds. (Usually loaded with function load_datasets)\n",
    "        * points_per_cloud: number of points to be found in each point cloud. (Default is 200)\n",
    "    '''\n",
    "    d_x, d_y = [], []\n",
    "    for pc in data:\n",
    "        for i in range(0, len(pc), points_per_cloud): \n",
    "            if len(pc[i:i+points_per_cloud]) == points_per_cloud:\n",
    "                t_ = np.array(pc[i:i+points_per_cloud])[:, :3]\n",
    "                d_x.append(t_)\n",
    "                d_y.append(np.array(pc[i:i+points_per_cloud], dtype=np.float32)[:, -1])\n",
    "                # d_y.append(tf.one_hot(np.array(pc[i:i+points_per_cloud], dtype=np.float32)[:, -1], 2)) # One Hotted\n",
    "    d_x, d_y = np.stack(d_x), np.stack(d_y)\n",
    "    return d_x, d_y\n",
    "\n",
    "dataset = load_datasets(run_path)\n",
    "# Shuffle point cloud dataset\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Separate Train and Test data\n",
    "d_len = int(len(dataset)*0.7)\n",
    "train, test = dataset[:d_len], dataset[d_len:]\n",
    "\n",
    "# Get X and Y data for training\n",
    "train_x, train_y = get_data_and_label(train)\n",
    "test_x, test_y = get_data_and_label(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Graph CNN \n",
    "#### https://github.com/WangYueFt/dgcnn\n",
    "#### https://stackoverflow.com/questions/37009647/compute-pairwise-distance-in-a-batch-without-replicating-tensor-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_conv2d(inputs,\n",
    "            filters,\n",
    "            kernel_size,\n",
    "            stride=[1, 1],\n",
    "            padding='SAME',\n",
    "            use_xavier=True,\n",
    "            stddev=1e-3,\n",
    "            activation_fn=tf.nn.elu,\n",
    "            bn=False):\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters, \n",
    "        kernel_size, \n",
    "        strides=stride, \n",
    "        padding=padding,\n",
    "        activation=activation_fn,\n",
    "        kernel_initializer='glorot_uniform' if use_xavier else keras.initializers.TruncatedNormal(stddev=stddev),\n",
    "        bias_initializer='zeros'\n",
    "    )(inputs)\n",
    "\n",
    "    if bn: x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    return x\n",
    "\n",
    "def gnn_dense(inputs,\n",
    "            units,\n",
    "            use_xavier=True,\n",
    "            stddev=1e-3,\n",
    "            activation_fn=tf.nn.elu,\n",
    "            bn=False):\n",
    "            \n",
    "    x = layers.Dense(units,\n",
    "        activation=activation_fn,\n",
    "        kernel_initializer='glorot_uniform' if use_xavier else keras.initializers.TruncatedNormal(stddev=stddev),\n",
    "        bias_initializer='zeros'\n",
    "    )(inputs)\n",
    "\n",
    "    if bn: x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    return x\n",
    "\n",
    "def lambda_get_adj_matr(input):\n",
    "    pcT = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(input)\n",
    "    pc_inn = layers.Lambda(lambda x: tf.matmul(x[0], x[1]))( (input, pcT) )\n",
    "    pc2 = layers.Lambda(lambda x: tf.reduce_sum(tf.square(x), axis=-1, keepdims=True))(input)\n",
    "    pc2T = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(pc2)\n",
    "    output = layers.Lambda(lambda x: x[0] + -2 * x[1] + x[2])( (pc2, pc_inn, pc2T) )\n",
    "    # Uncomment line below to use reciprocal of adj matrix (1/distance)\n",
    "    # output = layers.Lambda(lambda x: tf.math.reciprocal(x))(output)\n",
    "    return output\n",
    "\n",
    "def lambda_knn(adj, k=20):\n",
    "    x = layers.Lambda(lambda x: tf.math.top_k(-x[0], x[1]))( (adj, k) )\n",
    "    return x.indices\n",
    "\n",
    "def lambda_edge_feature(inputs, nn_idxs, k=20, num_points=200, num_dims=3):\n",
    "\n",
    "    pc_central = inputs\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    idx_ = layers.Lambda(lambda x: tf.range(x[0]) * x[1])( (batch_size, num_points) )\n",
    "    idx_ = layers.Lambda(lambda x: tf.reshape(x[0], (x[1], 1, 1)))( (idx_, batch_size) )\n",
    "    # Adding to list of idxs of k points the points themselves\n",
    "    pc_temp1 = layers.Lambda(lambda x: x[0]+x[1])( (nn_idxs, idx_) )\n",
    "\n",
    "    # Flattening of points into a list of coordinates (x,y,z)\n",
    "    pc_flat = layers.Lambda(lambda x: tf.reshape(x[0], [-1, x[1]]))( (inputs, num_dims) )\n",
    "\n",
    "    # Collect points from computed idxs\n",
    "    pc_neighbors = layers.Lambda(lambda x: tf.gather(x[0], x[1]) )( (pc_flat, pc_temp1) )\n",
    "\n",
    "    # Reshape points into shape (batch, num_points, NEW_AXIS = 1, num_dims)\n",
    "    pc_central = layers.Lambda(lambda x: tf.expand_dims(x, axis=-2))(pc_central)\n",
    "    # Points are repeated k-times along new dimension ==> (batch, num_points, k, num_dims)\n",
    "    pc_central = layers.Lambda(lambda x: tf.tile(x[0], [1, 1, x[1], 1]))( (pc_central, k) )\n",
    "\n",
    "    pc_temp2 = layers.Lambda(lambda x: tf.subtract(x[0], x[1]))( (pc_neighbors, pc_central) )\n",
    "    edge_feature = layers.Lambda(lambda x: tf.concat((x[0], x[1]), axis=-1))((pc_central, pc_temp2))\n",
    "    return edge_feature\n",
    "\n",
    "def gnn_tnet(inputs, num_dims, tnet_shapes, bn=False):\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "    for filt in tnet_shapes[0]:\n",
    "        x = gnn_conv2d(inputs, filters=filt, kernel_size=[1,1], bn=bn)\n",
    "    x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "    for filt in tnet_shapes[1]:\n",
    "        x = gnn_conv2d(inputs, filters=filt, kernel_size=[1,1], bn=bn)\n",
    "    x = layers.GlobalMaxPooling2D(keepdims=True)(x)\n",
    "    x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], y[2])))( [x, batch_size, x.shape[-1]] )\n",
    "\n",
    "    for neur in tnet_shapes[2]:\n",
    "        x = gnn_dense(x, neur, bn)\n",
    "    \n",
    "    bias = keras.initializers.Constant(np.eye(num_dims).flatten())\n",
    "    x = layers.Dense(\n",
    "        num_dims * num_dims,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_dims, num_dims))(x)\n",
    "    return feat_T\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "# test_name = \"test_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test custom losses\n",
    "def custom_loss(pred, labels):\n",
    "    # loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=labels, logits=pred, label_smoothing=0.2)\n",
    "    # classify_loss = tf.reduce_mean(loss)\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.math.square(tf.math.subtract(labels, pred)), axis=-1))\n",
    "    return loss\n",
    "\n",
    "# Callback to save good models. Threshold is on validation accuracy.\n",
    "class ValAccThresh_CB(keras.callbacks.Callback):\n",
    "    def __init__(self, thresh=0.9):\n",
    "        self.thresh = thresh\n",
    "        super(keras.callbacks.Callback, self).__init__()\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "        self.stopped_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"val_sparse_categorical_accuracy\")\n",
    "        # current = logs.get(\"val_accuracy\")\n",
    "        if current >= self.thresh:\n",
    "            self.thresh = current\n",
    "            self.model.set_weights(experiments_path+test_name+\"/best_weights/cp-\"+str(epoch)+\".ckpt\")\n",
    "            print(\" New good model saved.\")\n",
    "\n",
    "# Callback to save history for post-processing\n",
    "# filename=experiments_path+test_name+\"/history.csv\"\n",
    "# history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(inputs, \n",
    "        num_points, num_dims, k,\n",
    "        tnet_shape,\n",
    "        conv_gnns,\n",
    "        dense_gnn,\n",
    "        classes=2):\n",
    "    '''\n",
    "        Returns the outputs of the model to be compiled.\n",
    "        ---\n",
    "        Arguments: \n",
    "        * inputs:       Expected (None, 3). instance of tf.Input.\n",
    "        * num_points:   Number of points per point cloud. Default is 200\n",
    "        * num_dims:     Number of dimensions per point. Default is 3 (x, y, z)\n",
    "        * k:            K nearest neighbors\n",
    "        * tnet_shape:   Array of three lists. (each list's length is the number of layers for that section)\n",
    "                        1st is a list of filters for convolutional layers before reduce_max.\n",
    "                        2nd is a list of filters for convolutional layers after reduce_max.\n",
    "                        3rd is a list of neurons for dense layers after max pooling.\n",
    "        * conv_gnns:    list. Each row is composed of two lists.\n",
    "                        1st is a list of filters for convolutional layers before computing edge features.\n",
    "                        2nd is a list of filters for convolutional layers after computing edge features.\n",
    "        * dense_gnn:    list of neuorns for dense layers at the end of the network.\n",
    "        * classes:      number of classes to classify.\n",
    "    '''\n",
    "\n",
    "    adj = lambda_get_adj_matr(inputs)\n",
    "    nn_idxs = lambda_knn(adj, k)\n",
    "    edge_feats = lambda_edge_feature(inputs, nn_idxs, k, num_points, num_dims)\n",
    "    feat_T = gnn_tnet(edge_feats, num_dims, tnet_shape, bn=True)\n",
    "    pc_tf = layers.Dot(axes=(-1, -2))([inputs, feat_T]) # Apply affine transformation to input features\n",
    "\n",
    "    adj = lambda_get_adj_matr(pc_tf)\n",
    "    nn_idxs = lambda_knn(adj, k)\n",
    "    edge_feats = lambda_edge_feature(pc_tf, nn_idxs, k, num_points, num_dims)\n",
    "\n",
    "    for l in conv_gnns:\n",
    "        x = edge_feats\n",
    "        for gc_filt in l[0]:\n",
    "            x = gnn_conv2d(x, gc_filt, [1,1], bn=True)\n",
    "        x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "        x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[0][-1])))( [x, tf.shape(inputs)[0]] )\n",
    "\n",
    "        adj = lambda_get_adj_matr(x)\n",
    "        nn_idxs = lambda_knn(adj, k)\n",
    "        edge_feats = lambda_edge_feature(x, nn_idxs, k, num_points, l[0][-1])\n",
    "        x = edge_feats\n",
    "        for gc_filt in l[1]:\n",
    "            x = gnn_conv2d(x, gc_filt, [1,1], bn=True)\n",
    "        x = tf.reduce_max(x, axis=-2, keepdims=True)\n",
    "        x = layers.Lambda(lambda y: tf.reshape(y[0], (y[1], num_points, l[1][-1])))( [x, tf.shape(inputs)[0]] )\n",
    "\n",
    "    for w_ in dense_gnn:\n",
    "        x = gnn_dense(x, w_, bn=True)\n",
    "\n",
    "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    return outputs\n",
    "\n",
    "def objective(trial):\n",
    "    test_name = \"test_\"+str(trial.number)\n",
    "    filename=experiments_path+test_name+\"/history.csv\"\n",
    "    history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "    ############################ HyperParameter Setup ############################\n",
    "    ######################### Check build_model for docs #########################\n",
    "    k = trial.suggest_int('k', 5,75) #30\n",
    "    batch_size = trial.suggest_int('batch_size', 8,128) #16\n",
    "    tnet_before_max = trial.suggest_int('tnet_before_max', 1,3)\n",
    "    tnet_before = []\n",
    "    for i in range(tnet_before_max):\n",
    "        tnet_before.append(trial.suggest_int('tnet_beforemax_layer_'+str(i), 8,128))\n",
    "    tnet_after = []\n",
    "    tnet_after_max = trial.suggest_int('tnet_after_max', 1,3)\n",
    "    for i in range(tnet_after_max):\n",
    "        tnet_after.append(trial.suggest_int('tnet_aftermax_layer_'+str(i), 8,128))\n",
    "    tnet_dense = []\n",
    "    tnet_dense_layers = trial.suggest_int('tnet_dense_layers', 1,3)\n",
    "    for i in range(tnet_dense_layers):\n",
    "        tnet_dense.append(trial.suggest_int('tnet_dense_layer_'+str(i), 16,256))\n",
    "    tnet_shape = [tnet_before, tnet_after, tnet_dense]\n",
    "\n",
    "    gc_layers = trial.suggest_int('gc_layers', 1,3) #1\n",
    "    conv_gnns = []\n",
    "    for _ in range(gc_layers):\n",
    "        before_edge_gcl = trial.suggest_int('before_edge_gcl', 1,3) #2\n",
    "        after_edge_gcl = trial.suggest_int('after_edge_gcl', 1,3) #2\n",
    "\n",
    "        bfr_edge = []\n",
    "        for i in range(before_edge_gcl):\n",
    "            bfr_edge.append(trial.suggest_int('before_edge_gcl_'+str(i), 8,128))\n",
    "            \n",
    "        aft_edge = []\n",
    "        for i in range(after_edge_gcl):\n",
    "            aft_edge.append(trial.suggest_int('after_edge_gcl_'+str(i), 8,128))\n",
    "            \n",
    "        conv_gnns.append([bfr_edge, aft_edge])\n",
    "    dense_layers = trial.suggest_int('dense_layers', 1,3)\n",
    "    dense_gnn = []\n",
    "    for i in range(dense_layers):\n",
    "        dense_gnn.append(trial.suggest_int('dense_layer_'+str(i), 16,256))\n",
    "        \n",
    "    lr = trial.suggest_float('lr', 0.0001, 0.1)\n",
    "    steps_per_epoch=trial.suggest_int('steps_per_epoch', 15,100)\n",
    "\n",
    "    validation_steps=25     # Static\n",
    "    num_points = 200        # Static\n",
    "    num_dims = 3            # Static\n",
    "    ##############################################################################\n",
    "\n",
    "    inputs = keras.Input(shape=(None, num_dims))\n",
    "    \n",
    "    outputs = build_model(inputs,\n",
    "                    num_points, num_dims, k,\n",
    "                    tnet_shape,\n",
    "                    conv_gnns,\n",
    "                    dense_gnn\n",
    "                )\n",
    "    model = keras.Model(inputs=[inputs], outputs=outputs, name=\"gnn_pointnet\")\n",
    "\n",
    "    opt_pi = tf.optimizers.Adam(learning_rate =  lr )\n",
    "    # opt_pi = tf.optimizers.RMSprop(learning_rate =  lr )\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=opt_pi, metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    # model.compile(loss=tf.nn.sparse_softmax_cross_entropy_with_logits , optimizer=opt_pi, metrics=['accuracy'])\n",
    "    # model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=opt_pi, metrics=['accuracy'])\n",
    "    # model.compile(loss=custom_loss, optimizer=opt_pi, metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_path = experiments_path+test_name+\"/cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, \n",
    "        verbose=0, \n",
    "        save_weights_only=True,\n",
    "        save_freq=10*batch_size)\n",
    "        \n",
    "    latest = tf.train.latest_checkpoint(experiments_path+test_name+\"/\")\n",
    "    if latest:\n",
    "        model.load_weights(latest)\n",
    "        latest_ep = int(latest.split('/')[-1].split('-')[-1].split('.')[0])\n",
    "        print(\" Model loaded correctly:\", latest, \" - Epoch \", latest_ep)\n",
    "    else:\n",
    "        print(\" The model could not be loaded properly: \", latest)\n",
    "        model.save_weights(checkpoint_path.format(epoch=0))\n",
    "        latest_ep = 0\n",
    "\n",
    "    history = model.fit(\n",
    "        train_x, \n",
    "        train_y, \n",
    "        \n",
    "        initial_epoch=latest_ep,\n",
    "        batch_size=batch_size, \n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "\n",
    "        validation_split=0.3,\n",
    "        epochs=150 - latest_ep, # Train for 150 epochs to find the configuration that can later be trained for more epochs.\n",
    "        shuffle=True,\n",
    "        callbacks=[ValAccThresh_CB(thresh=0.9), cp_callback, history_logger],\n",
    "        use_multiprocessing=False,\n",
    "        workers=8,\n",
    "    )\n",
    "    return np.mean(history.history['val_sparse_categorical_accuracy'][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually train\n",
    "\n",
    "storage = optuna.storages.RDBStorage(url=\"sqlite:///gnn.db\", engine_kwargs={\"connect_args\": {\"timeout\": 5}})\n",
    "study = optuna.create_study(study_name=\"gnn_denoising\", storage=storage, load_if_exists=True, direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
